{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DUNET_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJD3i4K-M44G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6ca97a-2412-4d8d-d618-4e9c4121c5e4"
      },
      "source": [
        "!pip3 install kymatio\n",
        "!pip3 install scikit-cuda\n",
        "!pip install torch==1.7.0 torchvision==0.8.0\n",
        "import numpy as np \n",
        "import os,csv\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchsummary import summary\n",
        "import random\n",
        "from torchvision import transforms\n",
        "import torch.optim\n",
        "from PIL import Image\n",
        "from kymatio.torch import Scattering2D"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kymatio in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kymatio) (20.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.1)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.7/dist-packages (from kymatio) (5.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.19.5)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from kymatio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kymatio) (2.4.7)\n",
            "Requirement already satisfied: scikit-cuda in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: mako>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-cuda) (1.1.4)\n",
            "Requirement already satisfied: pycuda>=2016.1 in /usr/local/lib/python3.7/dist-packages (from scikit-cuda) (2020.1)\n",
            "Requirement already satisfied: numpy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-cuda) (1.19.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from mako>=1.0.1->scikit-cuda) (1.1.1)\n",
            "Requirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from pycuda>=2016.1->scikit-cuda) (1.4.4)\n",
            "Requirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pycuda>=2016.1->scikit-cuda) (4.4.2)\n",
            "Requirement already satisfied: pytools>=2011.2 in /usr/local/lib/python3.7/dist-packages (from pycuda>=2016.1->scikit-cuda) (2021.2.2)\n",
            "Collecting torch==1.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting torchvision==0.8.0\n",
            "  Using cached https://files.pythonhosted.org/packages/1d/3f/4f45249458a0dee85bff7acf4a2ac6177708253f1f318fcf6ee230fb864f/torchvision-0.8.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed torch-1.7.0 torchvision-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmN017v1NII2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247368f0-948e-46d3-9d0a-557944f8d716"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfEADlnAyEj3"
      },
      "source": [
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 6\n",
        "LEARNING_RATE = 1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYB0sPsryqRR"
      },
      "source": [
        "def getImageName(filename):\n",
        "    imageName = filename[:-4] + '_Annotation.png'\n",
        "    return imageName\n",
        "\n",
        "# scattering = Scattering2D(J=2, shape=(544, 800), backend='torch_skcuda')\n",
        "scattering = Scattering2D(J=2, shape=(544, 800))\n",
        "scattering.cuda()\n",
        "\n",
        "def scatter_img(img):\n",
        "  return scattering(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNXJ6aMzGK3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788032d6-9369-4cc7-d601-dc5ed25373e8"
      },
      "source": [
        "x = torch.randn(1,544,800)\n",
        "b = scattering(x.cuda()).squeeze(dim=0)\n",
        "print(b.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([81, 136, 200])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/kymatio/scattering2d/backend/torch_backend.py:148: UserWarning: The function torch.fft is deprecated and will be removed in PyTorch 1.8. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.fftn. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:567.)\n",
            "  fft = FFT(lambda x: torch.fft(x, 2, normalized=False),\n",
            "/usr/local/lib/python3.7/dist-packages/kymatio/scattering2d/backend/torch_backend.py:150: UserWarning: The function torch.irfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.ifft or torch.fft.irfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:602.)\n",
            "  lambda x: torch.irfft(x, 2, normalized=False, onesided=False),\n",
            "/usr/local/lib/python3.7/dist-packages/kymatio/scattering2d/backend/torch_backend.py:149: UserWarning: The function torch.ifft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.ifft or torch.fft.ifftn. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:578.)\n",
            "  lambda x: torch.ifft(x, 2, normalized=False),\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrPLU36Vw-DH"
      },
      "source": [
        "class HeadDataset(Dataset):\n",
        "    def __init__(self, image_path, mask_path, files, transform=None):\n",
        "        self.image_path = image_path\n",
        "        self.mask_path = mask_path\n",
        "        self.files = files\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "            \n",
        "        \n",
        "        img_name = os.path.join(self.image_path,self.files[idx])\n",
        "        mask_name = os.path.join(self.mask_path,getImageName(self.files[idx]))\n",
        "        \n",
        "        image = Image.open(img_name).convert('L')\n",
        "        mask = Image.open(mask_name).convert('L')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "        image = image.to(device=device)\n",
        "        mask = mask.to(device=device)\n",
        "        scattering = scatter_img(image).squeeze(dim=0).detach()\n",
        "        sample = {'image': image, 'mask': mask, 'scatter':scattering}\n",
        "        return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMyEeveuxArF"
      },
      "source": [
        "base_folder = '/content/drive/MyDrive/hc18/training_set'\n",
        "image_folder = base_folder\n",
        "mask_folder = base_folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtvSKb4p0HKq"
      },
      "source": [
        "traindir=\"/content/drive/MyDrive/hc18/training_set/\"\n",
        "i=0\n",
        "trainname=[]\n",
        "with open(\"/content/drive/MyDrive/hc18/training_set_pixel_size_and_HC.csv\",'r') as s:\n",
        "    r=csv.reader(s)\n",
        "    for com in r:\n",
        "        if i==0:\n",
        "            i+=1\n",
        "        else:\n",
        "            i+=1\n",
        "            trainname.append(com[0])\n",
        "files=trainname"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYbQs3Ur0rih"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((544, 800)),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaq_bcNaxI72"
      },
      "source": [
        "Head_Train_Dataset = HeadDataset(image_folder,mask_folder,files,transform)\n",
        "trainLoader = DataLoader(Head_Train_Dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
        "# DME_Val_Dataset = DMEDataset(val_frame_path,val_masks_path)\n",
        "# DME_Test_Dataset = DMEDataset(test_frame_path,test_masks_path)\n",
        "# valDataSet = DataLoader(DME_Val_Dataset, batch_size=1,shuffle=True)\n",
        "# testDataSet = DataLoader(DME_Test_Dataset, batch_size=1,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN5y1AnPxMcB"
      },
      "source": [
        "class diceloss(torch.nn.Module):\n",
        "    def init(self):\n",
        "        super(diceLoss, self).init()\n",
        "    def forward(self, target, pred):\n",
        "       smooth = 1.\n",
        "       iflat = pred.contiguous().view(-1)\n",
        "       tflat = target.contiguous().view(-1)\n",
        "       intersection = (iflat * tflat).sum()\n",
        "       A_sum = torch.sum(iflat * iflat)\n",
        "       B_sum = torch.sum(tflat * tflat)\n",
        "       return 1 - ((2. * intersection + smooth) / (A_sum + B_sum + smooth))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBwz-K8DpXgq"
      },
      "source": [
        "# class Unet(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Unet, self).__init__()\n",
        "       \n",
        "#         self.encoder_1 = nn.Sequential(\n",
        "#             nn.Conv2d(1, 64, 3, padding=1),   \n",
        "#             nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "#             nn.Conv2d(64, 64, 3, padding=1),   \n",
        "#             nn.MaxPool2d(2, stride=2), nn.ReLU(True)\n",
        "#         )\n",
        "#         self.encoder_2 = nn.Sequential(\n",
        "#             nn.Conv2d(64, 128, 3, padding=1),   \n",
        "#             nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "#             nn.Conv2d(128, 128, 3, padding=1),\n",
        "#             nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "#         )\n",
        "#         self.encoder_3 = nn.Sequential(\n",
        "#             nn.Conv2d(128, 256, 3, padding=1),\n",
        "#             nn.BatchNorm2d(256),   nn.ReLU(True),\n",
        "#             nn.Conv2d(256, 256, 3, padding=1),  \n",
        "#             nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "#         )\n",
        "#         self.encoder_4 = nn.Sequential(\n",
        "#             nn.Conv2d(256, 512, 3, padding=1),   \n",
        "#             nn.BatchNorm2d(512),  nn.ReLU(True),\n",
        "#             nn.Conv2d(512, 512, 3, padding=1),   \n",
        "#             nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "#         )\n",
        "#         self.encoder_5 = nn.Sequential(\n",
        "#             nn.Conv2d(512, 1024, 3, padding=1),   \n",
        "#             nn.BatchNorm2d(1024),   nn.ReLU(True),\n",
        "#             nn.Conv2d(1024, 1024, 3, padding=1),   \n",
        "#             nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "#         )\n",
        "#         self.conv_merge_1= nn.Sequential(\n",
        "#             nn.Conv2d(1024, 1024, 3, padding=1),   nn.ReLU(True)\n",
        "#         )\n",
        "#         self.conv_merge_2= nn.Sequential(\n",
        "#             nn.Conv2d(1024, 512, 3, padding=1),   nn.ReLU(True)\n",
        "#         )\n",
        "#         self.conv_merge_3= nn.Sequential(\n",
        "#             nn.Conv2d(512, 256, 3, padding=1),   nn.ReLU(True)\n",
        "#         )\n",
        "#         self.conv_merge_4= nn.Sequential(\n",
        "#             nn.Conv2d(256,128, 3, padding=1),   nn.ReLU(True)\n",
        "#         )\n",
        "#         self.conv_merge_5=nn.Sequential(\n",
        "#             nn.Conv2d(128,64, 3, padding=1),   nn.ReLU(True),\n",
        "#         )\n",
        "#         self.decoder_1 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(1024, 512, 2, stride=2), \n",
        "#             nn.Conv2d(512, 512, 3, padding=1),   nn.ReLU(True),\n",
        "#         )\n",
        "#         self.decoder_2 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(512, 256, 2, stride=2), \n",
        "#             nn.Conv2d(256, 256, 3, padding=1),   nn.ReLU(True),\n",
        "#         )\n",
        "#         self.decoder_3 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(256, 128, 2, stride=2), \n",
        "#             nn.Conv2d(128, 128, 3, padding=1),   nn.ReLU(True),\n",
        "#         )\n",
        "#         self.decoder_4 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(128, 64, 2, stride=2), \n",
        "#             nn.Conv2d(64, 64, 3, padding=1),   nn.ReLU(True),\n",
        "#         )\n",
        "#         self.decoder_5 = nn.Sequential(\n",
        "#             nn.ConvTranspose2d(64, 32, 2, stride=2), \n",
        "#             nn.Conv2d(32, 1, 3, padding=1),   nn.ReLU(True),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.encoder_1(x)\n",
        "#         x2 = self.encoder_2(x1)\n",
        "#         x3 = self.encoder_3(x2)\n",
        "#         x4 = self.encoder_4(x3)\n",
        "#         x5 = self.encoder_5(x4)\n",
        "#         # b = self.scatter(y)\n",
        "#         # c = torch.cat([x5,b],dim=1)\n",
        "#         # print(\"Model\", x5.shape, b.shape, c.shape)\n",
        "#         d = self.conv_merge_1(x5)\n",
        "#         y1 = self.decoder_1(d)\n",
        "#         f = torch.cat([x4,y1],dim=1)\n",
        "#         g = self.conv_merge_2(f)\n",
        "#         y2 = self.decoder_2(g)\n",
        "#         h = torch.cat([x3,y2],dim=1)\n",
        "#         i = self.conv_merge_3(h)\n",
        "#         y3 = self.decoder_3(i)\n",
        "#         j = torch.cat([x2,y3],dim=1)\n",
        "#         k = self.conv_merge_4(j)\n",
        "#         y4 = self.decoder_4(k)\n",
        "#         l = torch.cat([x1,y4],dim=1)\n",
        "#         m = self.conv_merge_5(l)\n",
        "#         n = self.decoder_5(m)\n",
        "#         return n\n",
        "\n",
        "# Unet_model = Unet().to(device=device)\n",
        "# summary(Unet_model, (1, 544, 800))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc8c8nNRxpmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b286bd-83a4-4e09-8e6e-7787eed2352e"
      },
      "source": [
        "class Dunet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Dunet, self).__init__()\n",
        "       \n",
        "        self.encoder_1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, padding=1),   \n",
        "            nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),   \n",
        "            nn.MaxPool2d(2, stride=2), nn.ReLU(True)\n",
        "        )\n",
        "        self.encoder_2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),   \n",
        "            nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "        )\n",
        "        self.encoder_3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),   nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),  \n",
        "            nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "        )\n",
        "        self.encoder_4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),   \n",
        "            nn.BatchNorm2d(512),  nn.ReLU(True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),   \n",
        "            nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "        )\n",
        "        self.encoder_5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, 3, padding=1),   \n",
        "            nn.BatchNorm2d(1024),   nn.ReLU(True),\n",
        "            nn.Conv2d(1024, 1024, 3, padding=1),   \n",
        "            nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "        )\n",
        "        self.scatter = nn.Sequential(\n",
        "            nn.Conv2d(81, 256, 3, padding=1),   \n",
        "            nn.BatchNorm2d(256), nn.ReLU(True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),   \n",
        "            nn.MaxPool2d(2, stride=2),   nn.ReLU(True),\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.BatchNorm2d(512),   nn.ReLU(True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),   \n",
        "            nn.MaxPool2d(2, stride=2),    nn.ReLU(True),\n",
        "            nn.Conv2d(512, 1024, 3, padding=1),\n",
        "            nn.BatchNorm2d(1024),   nn.ReLU(True),\n",
        "            nn.Conv2d(1024, 1024, 3, padding=1),   \n",
        "            nn.MaxPool2d(2, stride=2),  nn.ReLU(True)\n",
        "        )\n",
        "        self.conv_merge_1= nn.Sequential(\n",
        "            nn.Conv2d(2048, 1024, 3, padding=1),   nn.ReLU(True)\n",
        "        )\n",
        "        self.conv_merge_2= nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, 3, padding=1),   nn.ReLU(True)\n",
        "        )\n",
        "        self.conv_merge_3= nn.Sequential(\n",
        "            nn.Conv2d(512, 256, 3, padding=1),   nn.ReLU(True)\n",
        "        )\n",
        "        self.conv_merge_4= nn.Sequential(\n",
        "            nn.Conv2d(256,128, 3, padding=1),   nn.ReLU(True)\n",
        "        )\n",
        "        self.conv_merge_5=nn.Sequential(\n",
        "            nn.Conv2d(128,64, 3, padding=1),   nn.ReLU(True),\n",
        "        )\n",
        "        self.decoder_1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 512, 2, stride=2), \n",
        "            nn.Conv2d(512, 512, 3, padding=1),   nn.ReLU(True),\n",
        "        )\n",
        "        self.decoder_2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 2, stride=2), \n",
        "            nn.Conv2d(256, 256, 3, padding=1),   nn.ReLU(True),\n",
        "        )\n",
        "        self.decoder_3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 2, stride=2), \n",
        "            nn.Conv2d(128, 128, 3, padding=1),   nn.ReLU(True),\n",
        "        )\n",
        "        self.decoder_4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 2, stride=2), \n",
        "            nn.Conv2d(64, 64, 3, padding=1),   nn.ReLU(True),\n",
        "        )\n",
        "        self.decoder_5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 2, stride=2), \n",
        "            nn.Conv2d(32, 1, 3, padding=1),   nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x1 = self.encoder_1(x)\n",
        "        x2 = self.encoder_2(x1)\n",
        "        x3 = self.encoder_3(x2)\n",
        "        x4 = self.encoder_4(x3)\n",
        "        x5 = self.encoder_5(x4)\n",
        "        b = self.scatter(y)\n",
        "        c = torch.cat([x5,b],dim=1)\n",
        "        # print(\"Model\", x5.shape, b.shape, c.shape)\n",
        "        d = self.conv_merge_1(c)\n",
        "        y1 = self.decoder_1(d)\n",
        "        f = torch.cat([x4,y1],dim=1)\n",
        "        g = self.conv_merge_2(f)\n",
        "        y2 = self.decoder_2(g)\n",
        "        h = torch.cat([x3,y2],dim=1)\n",
        "        i = self.conv_merge_3(h)\n",
        "        y3 = self.decoder_3(i)\n",
        "        j = torch.cat([x2,y3],dim=1)\n",
        "        k = self.conv_merge_4(j)\n",
        "        y4 = self.decoder_4(k)\n",
        "        l = torch.cat([x1,y4],dim=1)\n",
        "        m = self.conv_merge_5(l)\n",
        "        n = self.decoder_5(m)\n",
        "        return n\n",
        "\n",
        "model = Dunet().to(device=device)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/hc18/saved_model29'))\n",
        "# summary(model, [(1, 544, 800),(81,136,200)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWiDdo00wrRs"
      },
      "source": [
        "criterion = diceloss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=LEARNING_RATE*0.1)\n",
        "# optimizer = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-8, momentum=0.9)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U87Q_daEXhb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883d762e-b756-44db-861a-c4122fda0173"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        images = data['image']\n",
        "        masks = data['mask']\n",
        "        scatters = data['scatter']\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(images, scatters)\n",
        "        loss = criterion(outputs, masks)\n",
        "        print(\"Mini - Batch \",i,\"Loss\", loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print('\\t Epoch Loss = [%d] loss: %.3f' % (epoch, running_loss))\n",
        "        running_loss = 0.0\n",
        "    if epoch % 10 == 4:\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/hc18/saved_model'+str(epoch))\n",
        "    \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mini - Batch  0 Loss 0.04379880428314209\n",
            "Mini - Batch  1 Loss 0.016244173049926758\n",
            "Mini - Batch  2 Loss 0.025011420249938965\n",
            "Mini - Batch  3 Loss 0.04900014400482178\n",
            "Mini - Batch  4 Loss 0.02027803659439087\n",
            "Mini - Batch  5 Loss 0.01528078317642212\n",
            "Mini - Batch  6 Loss 0.047748029232025146\n",
            "Mini - Batch  7 Loss 0.01777660846710205\n",
            "Mini - Batch  8 Loss 0.01975882053375244\n",
            "Mini - Batch  9 Loss 0.023231327533721924\n",
            "Mini - Batch  10 Loss 0.03340190649032593\n",
            "Mini - Batch  11 Loss 0.07660055160522461\n",
            "Mini - Batch  12 Loss 0.042962074279785156\n",
            "Mini - Batch  13 Loss 0.03015238046646118\n",
            "Mini - Batch  14 Loss 0.03789675235748291\n",
            "Mini - Batch  15 Loss 0.05014318227767944\n",
            "Mini - Batch  16 Loss 0.04870772361755371\n",
            "Mini - Batch  17 Loss 0.024454593658447266\n",
            "Mini - Batch  18 Loss 0.028190970420837402\n",
            "Mini - Batch  19 Loss 0.030886709690093994\n",
            "Mini - Batch  20 Loss 0.021636962890625\n",
            "Mini - Batch  21 Loss 0.14087456464767456\n",
            "Mini - Batch  22 Loss 0.031519412994384766\n",
            "Mini - Batch  23 Loss 0.03292655944824219\n",
            "Mini - Batch  24 Loss 0.05212944746017456\n",
            "Mini - Batch  25 Loss 0.08585816621780396\n",
            "Mini - Batch  26 Loss 0.024741411209106445\n",
            "Mini - Batch  27 Loss 0.03205835819244385\n",
            "Mini - Batch  28 Loss 0.03705114126205444\n",
            "Mini - Batch  29 Loss 0.0442122220993042\n",
            "Mini - Batch  30 Loss 0.040046632289886475\n",
            "Mini - Batch  31 Loss 0.02549302577972412\n",
            "Mini - Batch  32 Loss 0.026639163494110107\n",
            "Mini - Batch  33 Loss 0.026090562343597412\n",
            "Mini - Batch  34 Loss 0.023221611976623535\n",
            "Mini - Batch  35 Loss 0.02204817533493042\n",
            "Mini - Batch  36 Loss 0.051287710666656494\n",
            "Mini - Batch  37 Loss 0.031871914863586426\n",
            "Mini - Batch  38 Loss 0.030876457691192627\n",
            "Mini - Batch  39 Loss 0.03330892324447632\n",
            "Mini - Batch  40 Loss 0.024607539176940918\n",
            "Mini - Batch  41 Loss 0.03919363021850586\n",
            "Mini - Batch  42 Loss 0.02730858325958252\n",
            "Mini - Batch  43 Loss 0.05361616611480713\n",
            "Mini - Batch  44 Loss 0.04198956489562988\n",
            "Mini - Batch  45 Loss 0.02921813726425171\n",
            "Mini - Batch  46 Loss 0.027495980262756348\n",
            "Mini - Batch  47 Loss 0.023162841796875\n",
            "Mini - Batch  48 Loss 0.03448694944381714\n",
            "Mini - Batch  49 Loss 0.021722018718719482\n",
            "Mini - Batch  50 Loss 0.017978668212890625\n",
            "Mini - Batch  51 Loss 0.03412282466888428\n",
            "Mini - Batch  52 Loss 0.04110223054885864\n",
            "Mini - Batch  53 Loss 0.17878228425979614\n",
            "Mini - Batch  54 Loss 0.04411458969116211\n",
            "Mini - Batch  55 Loss 0.037395179271698\n",
            "Mini - Batch  56 Loss 0.020742297172546387\n",
            "Mini - Batch  57 Loss 0.06776601076126099\n",
            "Mini - Batch  58 Loss 0.027769386768341064\n",
            "Mini - Batch  59 Loss 0.035702526569366455\n",
            "Mini - Batch  60 Loss 0.028033077716827393\n",
            "Mini - Batch  61 Loss 0.02793288230895996\n",
            "Mini - Batch  62 Loss 0.03374505043029785\n",
            "Mini - Batch  63 Loss 0.03953629732131958\n",
            "Mini - Batch  64 Loss 0.032362937927246094\n",
            "Mini - Batch  65 Loss 0.017208099365234375\n",
            "Mini - Batch  66 Loss 0.029637038707733154\n",
            "Mini - Batch  67 Loss 0.03266143798828125\n",
            "Mini - Batch  68 Loss 0.03496682643890381\n",
            "Mini - Batch  69 Loss 0.03944116830825806\n",
            "Mini - Batch  70 Loss 0.039869606494903564\n",
            "Mini - Batch  71 Loss 0.048313021659851074\n",
            "Mini - Batch  72 Loss 0.030435562133789062\n",
            "Mini - Batch  73 Loss 0.050784170627593994\n",
            "Mini - Batch  74 Loss 0.024459123611450195\n",
            "Mini - Batch  75 Loss 0.025698840618133545\n",
            "Mini - Batch  76 Loss 0.038841962814331055\n",
            "Mini - Batch  77 Loss 0.03771078586578369\n",
            "Mini - Batch  78 Loss 0.04916304349899292\n",
            "Mini - Batch  79 Loss 0.01947087049484253\n",
            "Mini - Batch  80 Loss 0.021818816661834717\n",
            "Mini - Batch  81 Loss 0.03214728832244873\n",
            "Mini - Batch  82 Loss 0.023630142211914062\n",
            "Mini - Batch  83 Loss 0.018684864044189453\n",
            "Mini - Batch  84 Loss 0.021865546703338623\n",
            "Mini - Batch  85 Loss 0.025247514247894287\n",
            "Mini - Batch  86 Loss 0.1290377378463745\n",
            "Mini - Batch  87 Loss 0.036769866943359375\n",
            "Mini - Batch  88 Loss 0.18602126836776733\n",
            "Mini - Batch  89 Loss 0.0372883677482605\n",
            "Mini - Batch  90 Loss 0.03851598501205444\n",
            "Mini - Batch  91 Loss 0.04618239402770996\n",
            "Mini - Batch  92 Loss 0.04739159345626831\n",
            "Mini - Batch  93 Loss 0.03844672441482544\n",
            "Mini - Batch  94 Loss 0.03947913646697998\n",
            "Mini - Batch  95 Loss 0.027833104133605957\n",
            "Mini - Batch  96 Loss 0.028848707675933838\n",
            "Mini - Batch  97 Loss 0.03243154287338257\n",
            "Mini - Batch  98 Loss 0.038734495639801025\n",
            "Mini - Batch  99 Loss 0.02313917875289917\n",
            "Mini - Batch  100 Loss 0.02253115177154541\n",
            "Mini - Batch  101 Loss 0.0316472053527832\n",
            "Mini - Batch  102 Loss 0.028388619422912598\n",
            "Mini - Batch  103 Loss 0.025834739208221436\n",
            "Mini - Batch  104 Loss 0.02425140142440796\n",
            "Mini - Batch  105 Loss 0.044141948223114014\n",
            "Mini - Batch  106 Loss 0.11018478870391846\n",
            "Mini - Batch  107 Loss 0.045277416706085205\n",
            "Mini - Batch  108 Loss 0.027724027633666992\n",
            "Mini - Batch  109 Loss 0.027796149253845215\n",
            "Mini - Batch  110 Loss 0.024902641773223877\n",
            "Mini - Batch  111 Loss 0.022338509559631348\n",
            "Mini - Batch  112 Loss 0.01937943696975708\n",
            "Mini - Batch  113 Loss 0.02574676275253296\n",
            "Mini - Batch  114 Loss 0.02700597047805786\n",
            "Mini - Batch  115 Loss 0.02174621820449829\n",
            "Mini - Batch  116 Loss 0.025204956531524658\n",
            "Mini - Batch  117 Loss 0.02038753032684326\n",
            "Mini - Batch  118 Loss 0.04714912176132202\n",
            "Mini - Batch  119 Loss 0.025348186492919922\n",
            "Mini - Batch  120 Loss 0.04510831832885742\n",
            "Mini - Batch  121 Loss 0.029476404190063477\n",
            "Mini - Batch  122 Loss 0.06616848707199097\n",
            "Mini - Batch  123 Loss 0.02435857057571411\n",
            "Mini - Batch  124 Loss 0.02644050121307373\n",
            "Mini - Batch  125 Loss 0.07162302732467651\n",
            "Mini - Batch  126 Loss 0.036808669567108154\n",
            "Mini - Batch  127 Loss 0.01864844560623169\n",
            "Mini - Batch  128 Loss 0.034374773502349854\n",
            "Mini - Batch  129 Loss 0.04042553901672363\n",
            "Mini - Batch  130 Loss 0.029739320278167725\n",
            "Mini - Batch  131 Loss 0.035652756690979004\n",
            "Mini - Batch  132 Loss 0.06335586309432983\n",
            "Mini - Batch  133 Loss 0.03969603776931763\n",
            "Mini - Batch  134 Loss 0.030655205249786377\n",
            "Mini - Batch  135 Loss 0.04905945062637329\n",
            "Mini - Batch  136 Loss 0.030894219875335693\n",
            "Mini - Batch  137 Loss 0.04588139057159424\n",
            "Mini - Batch  138 Loss 0.03573715686798096\n",
            "Mini - Batch  139 Loss 0.024481475353240967\n",
            "Mini - Batch  140 Loss 0.1487753987312317\n",
            "Mini - Batch  141 Loss 0.034788668155670166\n",
            "Mini - Batch  142 Loss 0.060425400733947754\n",
            "Mini - Batch  143 Loss 0.01677250862121582\n",
            "Mini - Batch  144 Loss 0.0251501202583313\n",
            "Mini - Batch  145 Loss 0.01662731170654297\n",
            "Mini - Batch  146 Loss 0.022339999675750732\n",
            "Mini - Batch  147 Loss 0.01950126886367798\n",
            "Mini - Batch  148 Loss 0.022302865982055664\n",
            "Mini - Batch  149 Loss 0.039644718170166016\n",
            "Mini - Batch  150 Loss 0.02534562349319458\n",
            "Mini - Batch  151 Loss 0.048053741455078125\n",
            "Mini - Batch  152 Loss 0.021031081676483154\n",
            "Mini - Batch  153 Loss 0.06538856029510498\n",
            "Mini - Batch  154 Loss 0.035727739334106445\n",
            "Mini - Batch  155 Loss 0.04924213886260986\n",
            "Mini - Batch  156 Loss 0.024788975715637207\n",
            "Mini - Batch  157 Loss 0.04822945594787598\n",
            "Mini - Batch  158 Loss 0.025398671627044678\n",
            "Mini - Batch  159 Loss 0.018322646617889404\n",
            "Mini - Batch  160 Loss 0.01897907257080078\n",
            "Mini - Batch  161 Loss 0.031220972537994385\n",
            "Mini - Batch  162 Loss 0.016808390617370605\n",
            "Mini - Batch  163 Loss 0.07804065942764282\n",
            "Mini - Batch  164 Loss 0.018280446529388428\n",
            "Mini - Batch  165 Loss 0.04292410612106323\n",
            "Mini - Batch  166 Loss 0.013851761817932129\n",
            "\t Epoch Loss = [0] loss: 6.311\n",
            "Mini - Batch  0 Loss 0.03167229890823364\n",
            "Mini - Batch  1 Loss 0.031360745429992676\n",
            "Mini - Batch  2 Loss 0.03009694814682007\n",
            "Mini - Batch  3 Loss 0.025319814682006836\n",
            "Mini - Batch  4 Loss 0.03219956159591675\n",
            "Mini - Batch  5 Loss 0.02556830644607544\n",
            "Mini - Batch  6 Loss 0.05004233121871948\n",
            "Mini - Batch  7 Loss 0.03438091278076172\n",
            "Mini - Batch  8 Loss 0.025790691375732422\n",
            "Mini - Batch  9 Loss 0.030167996883392334\n",
            "Mini - Batch  10 Loss 0.02404332160949707\n",
            "Mini - Batch  11 Loss 0.01943027973175049\n",
            "Mini - Batch  12 Loss 0.03514748811721802\n",
            "Mini - Batch  13 Loss 0.03188657760620117\n",
            "Mini - Batch  14 Loss 0.020054638385772705\n",
            "Mini - Batch  15 Loss 0.033196330070495605\n",
            "Mini - Batch  16 Loss 0.03918725252151489\n",
            "Mini - Batch  17 Loss 0.02881312370300293\n",
            "Mini - Batch  18 Loss 0.04422396421432495\n",
            "Mini - Batch  19 Loss 0.045917630195617676\n",
            "Mini - Batch  20 Loss 0.06273281574249268\n",
            "Mini - Batch  21 Loss 0.030061841011047363\n",
            "Mini - Batch  22 Loss 0.16664540767669678\n",
            "Mini - Batch  23 Loss 0.045080363750457764\n",
            "Mini - Batch  24 Loss 0.04656422138214111\n",
            "Mini - Batch  25 Loss 0.026446163654327393\n",
            "Mini - Batch  26 Loss 0.03779572248458862\n",
            "Mini - Batch  27 Loss 0.04888099431991577\n",
            "Mini - Batch  28 Loss 0.018601953983306885\n",
            "Mini - Batch  29 Loss 0.022925257682800293\n",
            "Mini - Batch  30 Loss 0.027249157428741455\n",
            "Mini - Batch  31 Loss 0.029979944229125977\n",
            "Mini - Batch  32 Loss 0.07560241222381592\n",
            "Mini - Batch  33 Loss 0.019340872764587402\n",
            "Mini - Batch  34 Loss 0.037504613399505615\n",
            "Mini - Batch  35 Loss 0.037322998046875\n",
            "Mini - Batch  36 Loss 0.016739726066589355\n",
            "Mini - Batch  37 Loss 0.02446216344833374\n",
            "Mini - Batch  38 Loss 0.027326107025146484\n",
            "Mini - Batch  39 Loss 0.029071450233459473\n",
            "Mini - Batch  40 Loss 0.027638375759124756\n",
            "Mini - Batch  41 Loss 0.027467727661132812\n",
            "Mini - Batch  42 Loss 0.04699748754501343\n",
            "Mini - Batch  43 Loss 0.027174532413482666\n",
            "Mini - Batch  44 Loss 0.023647010326385498\n",
            "Mini - Batch  45 Loss 0.03534424304962158\n",
            "Mini - Batch  46 Loss 0.021481096744537354\n",
            "Mini - Batch  47 Loss 0.02542245388031006\n",
            "Mini - Batch  48 Loss 0.019829928874969482\n",
            "Mini - Batch  49 Loss 0.03049468994140625\n",
            "Mini - Batch  50 Loss 0.15911632776260376\n",
            "Mini - Batch  51 Loss 0.026877224445343018\n",
            "Mini - Batch  52 Loss 0.02449589967727661\n",
            "Mini - Batch  53 Loss 0.03352797031402588\n",
            "Mini - Batch  54 Loss 0.041649699211120605\n",
            "Mini - Batch  55 Loss 0.02162313461303711\n",
            "Mini - Batch  56 Loss 0.024519920349121094\n",
            "Mini - Batch  57 Loss 0.028142273426055908\n",
            "Mini - Batch  58 Loss 0.019663691520690918\n",
            "Mini - Batch  59 Loss 0.08292317390441895\n",
            "Mini - Batch  60 Loss 0.049415767192840576\n",
            "Mini - Batch  61 Loss 0.04042762517929077\n",
            "Mini - Batch  62 Loss 0.021777749061584473\n",
            "Mini - Batch  63 Loss 0.021635770797729492\n",
            "Mini - Batch  64 Loss 0.022000610828399658\n",
            "Mini - Batch  65 Loss 0.07815665006637573\n",
            "Mini - Batch  66 Loss 0.021148085594177246\n",
            "Mini - Batch  67 Loss 0.01777893304824829\n",
            "Mini - Batch  68 Loss 0.05358290672302246\n",
            "Mini - Batch  69 Loss 0.020501673221588135\n",
            "Mini - Batch  70 Loss 0.023541569709777832\n",
            "Mini - Batch  71 Loss 0.03516244888305664\n",
            "Mini - Batch  72 Loss 0.024951696395874023\n",
            "Mini - Batch  73 Loss 0.04797250032424927\n",
            "Mini - Batch  74 Loss 0.03003561496734619\n",
            "Mini - Batch  75 Loss 0.03754580020904541\n",
            "Mini - Batch  76 Loss 0.03809702396392822\n",
            "Mini - Batch  77 Loss 0.03440362215042114\n",
            "Mini - Batch  78 Loss 0.023327529430389404\n",
            "Mini - Batch  79 Loss 0.026454269886016846\n",
            "Mini - Batch  80 Loss 0.03234279155731201\n",
            "Mini - Batch  81 Loss 0.034314095973968506\n",
            "Mini - Batch  82 Loss 0.022912025451660156\n",
            "Mini - Batch  83 Loss 0.03233683109283447\n",
            "Mini - Batch  84 Loss 0.040770888328552246\n",
            "Mini - Batch  85 Loss 0.056622326374053955\n",
            "Mini - Batch  86 Loss 0.059917569160461426\n",
            "Mini - Batch  87 Loss 0.032082974910736084\n",
            "Mini - Batch  88 Loss 0.03088432550430298\n",
            "Mini - Batch  89 Loss 0.05693376064300537\n",
            "Mini - Batch  90 Loss 0.026845812797546387\n",
            "Mini - Batch  91 Loss 0.024946212768554688\n",
            "Mini - Batch  92 Loss 0.021679461002349854\n",
            "Mini - Batch  93 Loss 0.029883205890655518\n",
            "Mini - Batch  94 Loss 0.030653774738311768\n",
            "Mini - Batch  95 Loss 0.04552018642425537\n",
            "Mini - Batch  96 Loss 0.023937463760375977\n",
            "Mini - Batch  97 Loss 0.021088242530822754\n",
            "Mini - Batch  98 Loss 0.02731013298034668\n",
            "Mini - Batch  99 Loss 0.10988271236419678\n",
            "Mini - Batch  100 Loss 0.026824533939361572\n",
            "Mini - Batch  101 Loss 0.021861374378204346\n",
            "Mini - Batch  102 Loss 0.034690022468566895\n",
            "Mini - Batch  103 Loss 0.02786862850189209\n",
            "Mini - Batch  104 Loss 0.02558654546737671\n",
            "Mini - Batch  105 Loss 0.01877152919769287\n",
            "Mini - Batch  106 Loss 0.016107797622680664\n",
            "Mini - Batch  107 Loss 0.03940802812576294\n",
            "Mini - Batch  108 Loss 0.022156715393066406\n",
            "Mini - Batch  109 Loss 0.024702847003936768\n",
            "Mini - Batch  110 Loss 0.02017354965209961\n",
            "Mini - Batch  111 Loss 0.036911189556121826\n",
            "Mini - Batch  112 Loss 0.024822711944580078\n",
            "Mini - Batch  113 Loss 0.022944271564483643\n",
            "Mini - Batch  114 Loss 0.031153559684753418\n",
            "Mini - Batch  115 Loss 0.016940414905548096\n",
            "Mini - Batch  116 Loss 0.033719420433044434\n",
            "Mini - Batch  117 Loss 0.02809804677963257\n",
            "Mini - Batch  118 Loss 0.05793219804763794\n",
            "Mini - Batch  119 Loss 0.0244905948638916\n",
            "Mini - Batch  120 Loss 0.0310019850730896\n",
            "Mini - Batch  121 Loss 0.01712721586227417\n",
            "Mini - Batch  122 Loss 0.029987812042236328\n",
            "Mini - Batch  123 Loss 0.028745174407958984\n",
            "Mini - Batch  124 Loss 0.038531720638275146\n",
            "Mini - Batch  125 Loss 0.023802995681762695\n",
            "Mini - Batch  126 Loss 0.021536707878112793\n",
            "Mini - Batch  127 Loss 0.02200406789779663\n",
            "Mini - Batch  128 Loss 0.03867948055267334\n",
            "Mini - Batch  129 Loss 0.034934937953948975\n",
            "Mini - Batch  130 Loss 0.038986384868621826\n",
            "Mini - Batch  131 Loss 0.022224247455596924\n",
            "Mini - Batch  132 Loss 0.06645679473876953\n",
            "Mini - Batch  133 Loss 0.01843804121017456\n",
            "Mini - Batch  134 Loss 0.03412598371505737\n",
            "Mini - Batch  135 Loss 0.020860373973846436\n",
            "Mini - Batch  136 Loss 0.02695631980895996\n",
            "Mini - Batch  137 Loss 0.02332019805908203\n",
            "Mini - Batch  138 Loss 0.026673078536987305\n",
            "Mini - Batch  139 Loss 0.060008347034454346\n",
            "Mini - Batch  140 Loss 0.03128582239151001\n",
            "Mini - Batch  141 Loss 0.03835701942443848\n",
            "Mini - Batch  142 Loss 0.04941827058792114\n",
            "Mini - Batch  143 Loss 0.016169846057891846\n",
            "Mini - Batch  144 Loss 0.020410120487213135\n",
            "Mini - Batch  145 Loss 0.024676382541656494\n",
            "Mini - Batch  146 Loss 0.04080742597579956\n",
            "Mini - Batch  147 Loss 0.017303645610809326\n",
            "Mini - Batch  148 Loss 0.03167080879211426\n",
            "Mini - Batch  149 Loss 0.04757964611053467\n",
            "Mini - Batch  150 Loss 0.04205876588821411\n",
            "Mini - Batch  151 Loss 0.07108044624328613\n",
            "Mini - Batch  152 Loss 0.03526359796524048\n",
            "Mini - Batch  153 Loss 0.022320330142974854\n",
            "Mini - Batch  154 Loss 0.024184823036193848\n",
            "Mini - Batch  155 Loss 0.04589080810546875\n",
            "Mini - Batch  156 Loss 0.02529531717300415\n",
            "Mini - Batch  157 Loss 0.03130871057510376\n",
            "Mini - Batch  158 Loss 0.019311130046844482\n",
            "Mini - Batch  159 Loss 0.020097553730010986\n",
            "Mini - Batch  160 Loss 0.04110252857208252\n",
            "Mini - Batch  161 Loss 0.026982009410858154\n",
            "Mini - Batch  162 Loss 0.1486714482307434\n",
            "Mini - Batch  163 Loss 0.05226165056228638\n",
            "Mini - Batch  164 Loss 0.028961539268493652\n",
            "Mini - Batch  165 Loss 0.021650314331054688\n",
            "Mini - Batch  166 Loss 0.048658549785614014\n",
            "\t Epoch Loss = [1] loss: 5.848\n",
            "Mini - Batch  0 Loss 0.0385819673538208\n",
            "Mini - Batch  1 Loss 0.03048616647720337\n",
            "Mini - Batch  2 Loss 0.028911590576171875\n",
            "Mini - Batch  3 Loss 0.020985722541809082\n",
            "Mini - Batch  4 Loss 0.021969735622406006\n",
            "Mini - Batch  5 Loss 0.028452515602111816\n",
            "Mini - Batch  6 Loss 0.04083484411239624\n",
            "Mini - Batch  7 Loss 0.053738951683044434\n",
            "Mini - Batch  8 Loss 0.04368013143539429\n",
            "Mini - Batch  9 Loss 0.11432087421417236\n",
            "Mini - Batch  10 Loss 0.07005810737609863\n",
            "Mini - Batch  11 Loss 0.030316650867462158\n",
            "Mini - Batch  12 Loss 0.01609748601913452\n",
            "Mini - Batch  13 Loss 0.09403067827224731\n",
            "Mini - Batch  14 Loss 0.016466081142425537\n",
            "Mini - Batch  15 Loss 0.03187793493270874\n",
            "Mini - Batch  16 Loss 0.03327828645706177\n",
            "Mini - Batch  17 Loss 0.030690491199493408\n",
            "Mini - Batch  18 Loss 0.02724921703338623\n",
            "Mini - Batch  19 Loss 0.13359922170639038\n",
            "Mini - Batch  20 Loss 0.02552652359008789\n",
            "Mini - Batch  21 Loss 0.04646778106689453\n",
            "Mini - Batch  22 Loss 0.04933583736419678\n",
            "Mini - Batch  23 Loss 0.026122868061065674\n",
            "Mini - Batch  24 Loss 0.02817988395690918\n",
            "Mini - Batch  25 Loss 0.02136850357055664\n",
            "Mini - Batch  26 Loss 0.032434284687042236\n",
            "Mini - Batch  27 Loss 0.02909719944000244\n",
            "Mini - Batch  28 Loss 0.02395188808441162\n",
            "Mini - Batch  29 Loss 0.022481560707092285\n",
            "Mini - Batch  30 Loss 0.039871811866760254\n",
            "Mini - Batch  31 Loss 0.030681967735290527\n",
            "Mini - Batch  32 Loss 0.06638646125793457\n",
            "Mini - Batch  33 Loss 0.06078988313674927\n",
            "Mini - Batch  34 Loss 0.0573887825012207\n",
            "Mini - Batch  35 Loss 0.05077463388442993\n",
            "Mini - Batch  36 Loss 0.05237984657287598\n",
            "Mini - Batch  37 Loss 0.02720123529434204\n",
            "Mini - Batch  38 Loss 0.054351627826690674\n",
            "Mini - Batch  39 Loss 0.028185606002807617\n",
            "Mini - Batch  40 Loss 0.048517704010009766\n",
            "Mini - Batch  41 Loss 0.03540283441543579\n",
            "Mini - Batch  42 Loss 0.02639371156692505\n",
            "Mini - Batch  43 Loss 0.02282238006591797\n",
            "Mini - Batch  44 Loss 0.026732027530670166\n",
            "Mini - Batch  45 Loss 0.029098212718963623\n",
            "Mini - Batch  46 Loss 0.01744896173477173\n",
            "Mini - Batch  47 Loss 0.03686833381652832\n",
            "Mini - Batch  48 Loss 0.023010969161987305\n",
            "Mini - Batch  49 Loss 0.022890925407409668\n",
            "Mini - Batch  50 Loss 0.028900325298309326\n",
            "Mini - Batch  51 Loss 0.03135150671005249\n",
            "Mini - Batch  52 Loss 0.027217328548431396\n",
            "Mini - Batch  53 Loss 0.02978205680847168\n",
            "Mini - Batch  54 Loss 0.024040937423706055\n",
            "Mini - Batch  55 Loss 0.025775134563446045\n",
            "Mini - Batch  56 Loss 0.02489537000656128\n",
            "Mini - Batch  57 Loss 0.050579965114593506\n",
            "Mini - Batch  58 Loss 0.03296583890914917\n",
            "Mini - Batch  59 Loss 0.019276738166809082\n",
            "Mini - Batch  60 Loss 0.022671520709991455\n",
            "Mini - Batch  61 Loss 0.04614502191543579\n",
            "Mini - Batch  62 Loss 0.15722882747650146\n",
            "Mini - Batch  63 Loss 0.06051534414291382\n",
            "Mini - Batch  64 Loss 0.027711689472198486\n",
            "Mini - Batch  65 Loss 0.021591782569885254\n",
            "Mini - Batch  66 Loss 0.02525651454925537\n",
            "Mini - Batch  67 Loss 0.0273892879486084\n",
            "Mini - Batch  68 Loss 0.03522646427154541\n",
            "Mini - Batch  69 Loss 0.04136991500854492\n",
            "Mini - Batch  70 Loss 0.016917288303375244\n",
            "Mini - Batch  71 Loss 0.0234563946723938\n",
            "Mini - Batch  72 Loss 0.03555780649185181\n",
            "Mini - Batch  73 Loss 0.022863388061523438\n",
            "Mini - Batch  74 Loss 0.04440075159072876\n",
            "Mini - Batch  75 Loss 0.029716074466705322\n",
            "Mini - Batch  76 Loss 0.03340035676956177\n",
            "Mini - Batch  77 Loss 0.02820974588394165\n",
            "Mini - Batch  78 Loss 0.04058492183685303\n",
            "Mini - Batch  79 Loss 0.04850691556930542\n",
            "Mini - Batch  80 Loss 0.027968525886535645\n",
            "Mini - Batch  81 Loss 0.03790485858917236\n",
            "Mini - Batch  82 Loss 0.04134100675582886\n",
            "Mini - Batch  83 Loss 0.027765750885009766\n",
            "Mini - Batch  84 Loss 0.021785855293273926\n",
            "Mini - Batch  85 Loss 0.018847882747650146\n",
            "Mini - Batch  86 Loss 0.026482880115509033\n",
            "Mini - Batch  87 Loss 0.05648523569107056\n",
            "Mini - Batch  88 Loss 0.026096224784851074\n",
            "Mini - Batch  89 Loss 0.019030332565307617\n",
            "Mini - Batch  90 Loss 0.04608219861984253\n",
            "Mini - Batch  91 Loss 0.021253108978271484\n",
            "Mini - Batch  92 Loss 0.028387844562530518\n",
            "Mini - Batch  93 Loss 0.04788130521774292\n",
            "Mini - Batch  94 Loss 0.17686963081359863\n",
            "Mini - Batch  95 Loss 0.016956031322479248\n",
            "Mini - Batch  96 Loss 0.02265697717666626\n",
            "Mini - Batch  97 Loss 0.03316742181777954\n",
            "Mini - Batch  98 Loss 0.06975287199020386\n",
            "Mini - Batch  99 Loss 0.023788928985595703\n",
            "Mini - Batch  100 Loss 0.02890712022781372\n",
            "Mini - Batch  101 Loss 0.040062129497528076\n",
            "Mini - Batch  102 Loss 0.029183506965637207\n",
            "Mini - Batch  103 Loss 0.02050703763961792\n",
            "Mini - Batch  104 Loss 0.03489404916763306\n",
            "Mini - Batch  105 Loss 0.01873725652694702\n",
            "Mini - Batch  106 Loss 0.05706441402435303\n",
            "Mini - Batch  107 Loss 0.03420257568359375\n",
            "Mini - Batch  108 Loss 0.02176368236541748\n",
            "Mini - Batch  109 Loss 0.017072975635528564\n",
            "Mini - Batch  110 Loss 0.01728719472885132\n",
            "Mini - Batch  111 Loss 0.021795809268951416\n",
            "Mini - Batch  112 Loss 0.027177095413208008\n",
            "Mini - Batch  113 Loss 0.037025272846221924\n",
            "Mini - Batch  114 Loss 0.034088134765625\n",
            "Mini - Batch  115 Loss 0.01642543077468872\n",
            "Mini - Batch  116 Loss 0.018939733505249023\n",
            "Mini - Batch  117 Loss 0.026764333248138428\n",
            "Mini - Batch  118 Loss 0.03024345636367798\n",
            "Mini - Batch  119 Loss 0.027987182140350342\n",
            "Mini - Batch  120 Loss 0.030910849571228027\n",
            "Mini - Batch  121 Loss 0.02161318063735962\n",
            "Mini - Batch  122 Loss 0.04303640127182007\n",
            "Mini - Batch  123 Loss 0.027378737926483154\n",
            "Mini - Batch  124 Loss 0.01576131582260132\n",
            "Mini - Batch  125 Loss 0.021369874477386475\n",
            "Mini - Batch  126 Loss 0.024039506912231445\n",
            "Mini - Batch  127 Loss 0.03771364688873291\n",
            "Mini - Batch  128 Loss 0.02094137668609619\n",
            "Mini - Batch  129 Loss 0.03222942352294922\n",
            "Mini - Batch  130 Loss 0.02929621934890747\n",
            "Mini - Batch  131 Loss 0.02827244997024536\n",
            "Mini - Batch  132 Loss 0.025043904781341553\n",
            "Mini - Batch  133 Loss 0.018380343914031982\n",
            "Mini - Batch  134 Loss 0.017097949981689453\n",
            "Mini - Batch  135 Loss 0.027162492275238037\n",
            "Mini - Batch  136 Loss 0.023655295372009277\n",
            "Mini - Batch  137 Loss 0.035243332386016846\n",
            "Mini - Batch  138 Loss 0.03144329786300659\n",
            "Mini - Batch  139 Loss 0.022265613079071045\n",
            "Mini - Batch  140 Loss 0.021465957164764404\n",
            "Mini - Batch  141 Loss 0.021454334259033203\n",
            "Mini - Batch  142 Loss 0.03499019145965576\n",
            "Mini - Batch  143 Loss 0.016315698623657227\n",
            "Mini - Batch  144 Loss 0.03180956840515137\n",
            "Mini - Batch  145 Loss 0.04687917232513428\n",
            "Mini - Batch  146 Loss 0.06929183006286621\n",
            "Mini - Batch  147 Loss 0.04313838481903076\n",
            "Mini - Batch  148 Loss 0.02547919750213623\n",
            "Mini - Batch  149 Loss 0.03230726718902588\n",
            "Mini - Batch  150 Loss 0.028555333614349365\n",
            "Mini - Batch  151 Loss 0.03308814764022827\n",
            "Mini - Batch  152 Loss 0.029398024082183838\n",
            "Mini - Batch  153 Loss 0.03154677152633667\n",
            "Mini - Batch  154 Loss 0.024120330810546875\n",
            "Mini - Batch  155 Loss 0.024136602878570557\n",
            "Mini - Batch  156 Loss 0.030437707901000977\n",
            "Mini - Batch  157 Loss 0.026599645614624023\n",
            "Mini - Batch  158 Loss 0.024725258350372314\n",
            "Mini - Batch  159 Loss 0.018529534339904785\n",
            "Mini - Batch  160 Loss 0.024377822875976562\n",
            "Mini - Batch  161 Loss 0.030972182750701904\n",
            "Mini - Batch  162 Loss 0.017664611339569092\n",
            "Mini - Batch  163 Loss 0.02617722749710083\n",
            "Mini - Batch  164 Loss 0.04615384340286255\n",
            "Mini - Batch  165 Loss 0.023458242416381836\n",
            "Mini - Batch  166 Loss 0.013473987579345703\n",
            "\t Epoch Loss = [2] loss: 5.731\n",
            "Mini - Batch  0 Loss 0.017700910568237305\n",
            "Mini - Batch  1 Loss 0.024599194526672363\n",
            "Mini - Batch  2 Loss 0.04347717761993408\n",
            "Mini - Batch  3 Loss 0.02267444133758545\n",
            "Mini - Batch  4 Loss 0.01793891191482544\n",
            "Mini - Batch  5 Loss 0.025199830532073975\n",
            "Mini - Batch  6 Loss 0.049579083919525146\n",
            "Mini - Batch  7 Loss 0.06207847595214844\n",
            "Mini - Batch  8 Loss 0.02114856243133545\n",
            "Mini - Batch  9 Loss 0.029285967350006104\n",
            "Mini - Batch  10 Loss 0.01799774169921875\n",
            "Mini - Batch  11 Loss 0.03843653202056885\n",
            "Mini - Batch  12 Loss 0.0315592885017395\n",
            "Mini - Batch  13 Loss 0.018609941005706787\n",
            "Mini - Batch  14 Loss 0.05296063423156738\n",
            "Mini - Batch  15 Loss 0.05389589071273804\n",
            "Mini - Batch  16 Loss 0.020250916481018066\n",
            "Mini - Batch  17 Loss 0.021323025226593018\n",
            "Mini - Batch  18 Loss 0.0430755615234375\n",
            "Mini - Batch  19 Loss 0.027449727058410645\n",
            "Mini - Batch  20 Loss 0.02302229404449463\n",
            "Mini - Batch  21 Loss 0.02555251121520996\n",
            "Mini - Batch  22 Loss 0.0439678430557251\n",
            "Mini - Batch  23 Loss 0.03680461645126343\n",
            "Mini - Batch  24 Loss 0.03310060501098633\n",
            "Mini - Batch  25 Loss 0.0302390456199646\n",
            "Mini - Batch  26 Loss 0.02805238962173462\n",
            "Mini - Batch  27 Loss 0.02446204423904419\n",
            "Mini - Batch  28 Loss 0.031894803047180176\n",
            "Mini - Batch  29 Loss 0.026169896125793457\n",
            "Mini - Batch  30 Loss 0.021056413650512695\n",
            "Mini - Batch  31 Loss 0.029660284519195557\n",
            "Mini - Batch  32 Loss 0.03447073698043823\n",
            "Mini - Batch  33 Loss 0.046625494956970215\n",
            "Mini - Batch  34 Loss 0.019664764404296875\n",
            "Mini - Batch  35 Loss 0.021483182907104492\n",
            "Mini - Batch  36 Loss 0.021230876445770264\n",
            "Mini - Batch  37 Loss 0.09850037097930908\n",
            "Mini - Batch  38 Loss 0.022763431072235107\n",
            "Mini - Batch  39 Loss 0.018238425254821777\n",
            "Mini - Batch  40 Loss 0.034619927406311035\n",
            "Mini - Batch  41 Loss 0.019806742668151855\n",
            "Mini - Batch  42 Loss 0.04094797372817993\n",
            "Mini - Batch  43 Loss 0.031135618686676025\n",
            "Mini - Batch  44 Loss 0.035165607929229736\n",
            "Mini - Batch  45 Loss 0.019094884395599365\n",
            "Mini - Batch  46 Loss 0.02646416425704956\n",
            "Mini - Batch  47 Loss 0.02269226312637329\n",
            "Mini - Batch  48 Loss 0.03539770841598511\n",
            "Mini - Batch  49 Loss 0.029642105102539062\n",
            "Mini - Batch  50 Loss 0.03150498867034912\n",
            "Mini - Batch  51 Loss 0.046424806118011475\n",
            "Mini - Batch  52 Loss 0.027702510356903076\n",
            "Mini - Batch  53 Loss 0.036155104637145996\n",
            "Mini - Batch  54 Loss 0.02939140796661377\n",
            "Mini - Batch  55 Loss 0.0814361572265625\n",
            "Mini - Batch  56 Loss 0.02502727508544922\n",
            "Mini - Batch  57 Loss 0.028866112232208252\n",
            "Mini - Batch  58 Loss 0.022144019603729248\n",
            "Mini - Batch  59 Loss 0.0343974232673645\n",
            "Mini - Batch  60 Loss 0.03147625923156738\n",
            "Mini - Batch  61 Loss 0.02729576826095581\n",
            "Mini - Batch  62 Loss 0.021085023880004883\n",
            "Mini - Batch  63 Loss 0.03562653064727783\n",
            "Mini - Batch  64 Loss 0.023953020572662354\n",
            "Mini - Batch  65 Loss 0.04599648714065552\n",
            "Mini - Batch  66 Loss 0.026454508304595947\n",
            "Mini - Batch  67 Loss 0.02704489231109619\n",
            "Mini - Batch  68 Loss 0.023708820343017578\n",
            "Mini - Batch  69 Loss 0.01808297634124756\n",
            "Mini - Batch  70 Loss 0.023317575454711914\n",
            "Mini - Batch  71 Loss 0.027946949005126953\n",
            "Mini - Batch  72 Loss 0.039161741733551025\n",
            "Mini - Batch  73 Loss 0.026074767112731934\n",
            "Mini - Batch  74 Loss 0.05732464790344238\n",
            "Mini - Batch  75 Loss 0.03194308280944824\n",
            "Mini - Batch  76 Loss 0.02364259958267212\n",
            "Mini - Batch  77 Loss 0.052935004234313965\n",
            "Mini - Batch  78 Loss 0.04239314794540405\n",
            "Mini - Batch  79 Loss 0.019179701805114746\n",
            "Mini - Batch  80 Loss 0.02086329460144043\n",
            "Mini - Batch  81 Loss 0.02950805425643921\n",
            "Mini - Batch  82 Loss 0.04322707653045654\n",
            "Mini - Batch  83 Loss 0.029862165451049805\n",
            "Mini - Batch  84 Loss 0.02715456485748291\n",
            "Mini - Batch  85 Loss 0.026259422302246094\n",
            "Mini - Batch  86 Loss 0.01899617910385132\n",
            "Mini - Batch  87 Loss 0.03302896022796631\n",
            "Mini - Batch  88 Loss 0.03726404905319214\n",
            "Mini - Batch  89 Loss 0.04802882671356201\n",
            "Mini - Batch  90 Loss 0.023451566696166992\n",
            "Mini - Batch  91 Loss 0.02700895071029663\n",
            "Mini - Batch  92 Loss 0.03811049461364746\n",
            "Mini - Batch  93 Loss 0.03925567865371704\n",
            "Mini - Batch  94 Loss 0.04644852876663208\n",
            "Mini - Batch  95 Loss 0.01982945203781128\n",
            "Mini - Batch  96 Loss 0.020056486129760742\n",
            "Mini - Batch  97 Loss 0.018952250480651855\n",
            "Mini - Batch  98 Loss 0.022396206855773926\n",
            "Mini - Batch  99 Loss 0.016169607639312744\n",
            "Mini - Batch  100 Loss 0.020848751068115234\n",
            "Mini - Batch  101 Loss 0.0384981632232666\n",
            "Mini - Batch  102 Loss 0.016060590744018555\n",
            "Mini - Batch  103 Loss 0.031246960163116455\n",
            "Mini - Batch  104 Loss 0.030868351459503174\n",
            "Mini - Batch  105 Loss 0.019934237003326416\n",
            "Mini - Batch  106 Loss 0.020950376987457275\n",
            "Mini - Batch  107 Loss 0.03805023431777954\n",
            "Mini - Batch  108 Loss 0.021178364753723145\n",
            "Mini - Batch  109 Loss 0.018820762634277344\n",
            "Mini - Batch  110 Loss 0.028602778911590576\n",
            "Mini - Batch  111 Loss 0.01624000072479248\n",
            "Mini - Batch  112 Loss 0.046523451805114746\n",
            "Mini - Batch  113 Loss 0.02507561445236206\n",
            "Mini - Batch  114 Loss 0.06581002473831177\n",
            "Mini - Batch  115 Loss 0.023300349712371826\n",
            "Mini - Batch  116 Loss 0.03368961811065674\n",
            "Mini - Batch  117 Loss 0.04202461242675781\n",
            "Mini - Batch  118 Loss 0.1137779951095581\n",
            "Mini - Batch  119 Loss 0.034544408321380615\n",
            "Mini - Batch  120 Loss 0.035986900329589844\n",
            "Mini - Batch  121 Loss 0.03446674346923828\n",
            "Mini - Batch  122 Loss 0.027595698833465576\n",
            "Mini - Batch  123 Loss 0.03562605381011963\n",
            "Mini - Batch  124 Loss 0.03386920690536499\n",
            "Mini - Batch  125 Loss 0.05193835496902466\n",
            "Mini - Batch  126 Loss 0.02670884132385254\n",
            "Mini - Batch  127 Loss 0.020757615566253662\n",
            "Mini - Batch  128 Loss 0.028496086597442627\n",
            "Mini - Batch  129 Loss 0.11235952377319336\n",
            "Mini - Batch  130 Loss 0.02408468723297119\n",
            "Mini - Batch  131 Loss 0.019287288188934326\n",
            "Mini - Batch  132 Loss 0.05236935615539551\n",
            "Mini - Batch  133 Loss 0.03311675786972046\n",
            "Mini - Batch  134 Loss 0.02188873291015625\n",
            "Mini - Batch  135 Loss 0.02849602699279785\n",
            "Mini - Batch  136 Loss 0.02589935064315796\n",
            "Mini - Batch  137 Loss 0.04128289222717285\n",
            "Mini - Batch  138 Loss 0.1796889305114746\n",
            "Mini - Batch  139 Loss 0.12621665000915527\n",
            "Mini - Batch  140 Loss 0.03398096561431885\n",
            "Mini - Batch  141 Loss 0.029273271560668945\n",
            "Mini - Batch  142 Loss 0.029268264770507812\n",
            "Mini - Batch  143 Loss 0.0231667160987854\n",
            "Mini - Batch  144 Loss 0.03371775150299072\n",
            "Mini - Batch  145 Loss 0.034441232681274414\n",
            "Mini - Batch  146 Loss 0.023350656032562256\n",
            "Mini - Batch  147 Loss 0.026793956756591797\n",
            "Mini - Batch  148 Loss 0.02420145273208618\n",
            "Mini - Batch  149 Loss 0.02433234453201294\n",
            "Mini - Batch  150 Loss 0.026717841625213623\n",
            "Mini - Batch  151 Loss 0.029509365558624268\n",
            "Mini - Batch  152 Loss 0.024617493152618408\n",
            "Mini - Batch  153 Loss 0.043379247188568115\n",
            "Mini - Batch  154 Loss 0.02138209342956543\n",
            "Mini - Batch  155 Loss 0.039897143840789795\n",
            "Mini - Batch  156 Loss 0.06588524580001831\n",
            "Mini - Batch  157 Loss 0.05645477771759033\n",
            "Mini - Batch  158 Loss 0.023726284503936768\n",
            "Mini - Batch  159 Loss 0.019489645957946777\n",
            "Mini - Batch  160 Loss 0.01965707540512085\n",
            "Mini - Batch  161 Loss 0.032658398151397705\n",
            "Mini - Batch  162 Loss 0.028759360313415527\n",
            "Mini - Batch  163 Loss 0.07799017429351807\n",
            "Mini - Batch  164 Loss 0.14236575365066528\n",
            "Mini - Batch  165 Loss 0.05024677515029907\n",
            "Mini - Batch  166 Loss 0.06901425123214722\n",
            "\t Epoch Loss = [3] loss: 5.845\n",
            "Mini - Batch  0 Loss 0.029564380645751953\n",
            "Mini - Batch  1 Loss 0.021064698696136475\n",
            "Mini - Batch  2 Loss 0.0167691707611084\n",
            "Mini - Batch  3 Loss 0.0474778413772583\n",
            "Mini - Batch  4 Loss 0.023285210132598877\n",
            "Mini - Batch  5 Loss 0.050385892391204834\n",
            "Mini - Batch  6 Loss 0.04799538850784302\n",
            "Mini - Batch  7 Loss 0.035060107707977295\n",
            "Mini - Batch  8 Loss 0.02985900640487671\n",
            "Mini - Batch  9 Loss 0.0330924391746521\n",
            "Mini - Batch  10 Loss 0.02337789535522461\n",
            "Mini - Batch  11 Loss 0.04438871145248413\n",
            "Mini - Batch  12 Loss 0.1283888816833496\n",
            "Mini - Batch  13 Loss 0.016897380352020264\n",
            "Mini - Batch  14 Loss 0.02518641948699951\n",
            "Mini - Batch  15 Loss 0.01670736074447632\n",
            "Mini - Batch  16 Loss 0.019401073455810547\n",
            "Mini - Batch  17 Loss 0.020250141620635986\n",
            "Mini - Batch  18 Loss 0.1614629626274109\n",
            "Mini - Batch  19 Loss 0.021660268306732178\n",
            "Mini - Batch  20 Loss 0.027687668800354004\n",
            "Mini - Batch  21 Loss 0.018709838390350342\n",
            "Mini - Batch  22 Loss 0.029650986194610596\n",
            "Mini - Batch  23 Loss 0.023137688636779785\n",
            "Mini - Batch  24 Loss 0.04747915267944336\n",
            "Mini - Batch  25 Loss 0.01869499683380127\n",
            "Mini - Batch  26 Loss 0.04137915372848511\n",
            "Mini - Batch  27 Loss 0.1362290382385254\n",
            "Mini - Batch  28 Loss 0.02245032787322998\n",
            "Mini - Batch  29 Loss 0.03297770023345947\n",
            "Mini - Batch  30 Loss 0.0349726676940918\n",
            "Mini - Batch  31 Loss 0.026934921741485596\n",
            "Mini - Batch  32 Loss 0.031704843044281006\n",
            "Mini - Batch  33 Loss 0.023001134395599365\n",
            "Mini - Batch  34 Loss 0.029172420501708984\n",
            "Mini - Batch  35 Loss 0.047947585582733154\n",
            "Mini - Batch  36 Loss 0.017189323902130127\n",
            "Mini - Batch  37 Loss 0.026284873485565186\n",
            "Mini - Batch  38 Loss 0.044753074645996094\n",
            "Mini - Batch  39 Loss 0.03386735916137695\n",
            "Mini - Batch  40 Loss 0.06037181615829468\n",
            "Mini - Batch  41 Loss 0.02774357795715332\n",
            "Mini - Batch  42 Loss 0.024246573448181152\n",
            "Mini - Batch  43 Loss 0.05152660608291626\n",
            "Mini - Batch  44 Loss 0.03291553258895874\n",
            "Mini - Batch  45 Loss 0.07778596878051758\n",
            "Mini - Batch  46 Loss 0.028109610080718994\n",
            "Mini - Batch  47 Loss 0.047111690044403076\n",
            "Mini - Batch  48 Loss 0.019557952880859375\n",
            "Mini - Batch  49 Loss 0.02779090404510498\n",
            "Mini - Batch  50 Loss 0.021000683307647705\n",
            "Mini - Batch  51 Loss 0.024659574031829834\n",
            "Mini - Batch  52 Loss 0.018553495407104492\n",
            "Mini - Batch  53 Loss 0.024785935878753662\n",
            "Mini - Batch  54 Loss 0.01712244749069214\n",
            "Mini - Batch  55 Loss 0.031028151512145996\n",
            "Mini - Batch  56 Loss 0.04862302541732788\n",
            "Mini - Batch  57 Loss 0.028329789638519287\n",
            "Mini - Batch  58 Loss 0.03723794221878052\n",
            "Mini - Batch  59 Loss 0.03421586751937866\n",
            "Mini - Batch  60 Loss 0.037021100521087646\n",
            "Mini - Batch  61 Loss 0.04816240072250366\n",
            "Mini - Batch  62 Loss 0.017004191875457764\n",
            "Mini - Batch  63 Loss 0.01893681287765503\n",
            "Mini - Batch  64 Loss 0.08038586378097534\n",
            "Mini - Batch  65 Loss 0.021405935287475586\n",
            "Mini - Batch  66 Loss 0.02412623167037964\n",
            "Mini - Batch  67 Loss 0.026116132736206055\n",
            "Mini - Batch  68 Loss 0.024837851524353027\n",
            "Mini - Batch  69 Loss 0.04034799337387085\n",
            "Mini - Batch  70 Loss 0.01773887872695923\n",
            "Mini - Batch  71 Loss 0.02835327386856079\n",
            "Mini - Batch  72 Loss 0.025578081607818604\n",
            "Mini - Batch  73 Loss 0.024074673652648926\n",
            "Mini - Batch  74 Loss 0.033569276332855225\n",
            "Mini - Batch  75 Loss 0.03166687488555908\n",
            "Mini - Batch  76 Loss 0.0272333025932312\n",
            "Mini - Batch  77 Loss 0.027105510234832764\n",
            "Mini - Batch  78 Loss 0.02257823944091797\n",
            "Mini - Batch  79 Loss 0.022164344787597656\n",
            "Mini - Batch  80 Loss 0.028278648853302002\n",
            "Mini - Batch  81 Loss 0.020018041133880615\n",
            "Mini - Batch  82 Loss 0.03241807222366333\n",
            "Mini - Batch  83 Loss 0.02158176898956299\n",
            "Mini - Batch  84 Loss 0.017531633377075195\n",
            "Mini - Batch  85 Loss 0.07698899507522583\n",
            "Mini - Batch  86 Loss 0.02742820978164673\n",
            "Mini - Batch  87 Loss 0.019464552402496338\n",
            "Mini - Batch  88 Loss 0.01887434720993042\n",
            "Mini - Batch  89 Loss 0.02520740032196045\n",
            "Mini - Batch  90 Loss 0.057257652282714844\n",
            "Mini - Batch  91 Loss 0.03680896759033203\n",
            "Mini - Batch  92 Loss 0.04529917240142822\n",
            "Mini - Batch  93 Loss 0.03997927904129028\n",
            "Mini - Batch  94 Loss 0.02744501829147339\n",
            "Mini - Batch  95 Loss 0.03040379285812378\n",
            "Mini - Batch  96 Loss 0.04104894399642944\n",
            "Mini - Batch  97 Loss 0.028880417346954346\n",
            "Mini - Batch  98 Loss 0.026188135147094727\n",
            "Mini - Batch  99 Loss 0.04311853647232056\n",
            "Mini - Batch  100 Loss 0.020717203617095947\n",
            "Mini - Batch  101 Loss 0.030626237392425537\n",
            "Mini - Batch  102 Loss 0.03837227821350098\n",
            "Mini - Batch  103 Loss 0.023704886436462402\n",
            "Mini - Batch  104 Loss 0.017285406589508057\n",
            "Mini - Batch  105 Loss 0.031582415103912354\n",
            "Mini - Batch  106 Loss 0.025188803672790527\n",
            "Mini - Batch  107 Loss 0.021462082862854004\n",
            "Mini - Batch  108 Loss 0.023077428340911865\n",
            "Mini - Batch  109 Loss 0.03214907646179199\n",
            "Mini - Batch  110 Loss 0.07322227954864502\n",
            "Mini - Batch  111 Loss 0.021674633026123047\n",
            "Mini - Batch  112 Loss 0.03608351945877075\n",
            "Mini - Batch  113 Loss 0.04599404335021973\n",
            "Mini - Batch  114 Loss 0.032511353492736816\n",
            "Mini - Batch  115 Loss 0.0414469838142395\n",
            "Mini - Batch  116 Loss 0.01929265260696411\n",
            "Mini - Batch  117 Loss 0.035286903381347656\n",
            "Mini - Batch  118 Loss 0.04263472557067871\n",
            "Mini - Batch  119 Loss 0.04133892059326172\n",
            "Mini - Batch  120 Loss 0.03336840867996216\n",
            "Mini - Batch  121 Loss 0.1308671236038208\n",
            "Mini - Batch  122 Loss 0.03570789098739624\n",
            "Mini - Batch  123 Loss 0.025802969932556152\n",
            "Mini - Batch  124 Loss 0.030690133571624756\n",
            "Mini - Batch  125 Loss 0.030041635036468506\n",
            "Mini - Batch  126 Loss 0.020990729331970215\n",
            "Mini - Batch  127 Loss 0.036013782024383545\n",
            "Mini - Batch  128 Loss 0.03640824556350708\n",
            "Mini - Batch  129 Loss 0.04518955945968628\n",
            "Mini - Batch  130 Loss 0.03125184774398804\n",
            "Mini - Batch  131 Loss 0.01978135108947754\n",
            "Mini - Batch  132 Loss 0.05198854207992554\n",
            "Mini - Batch  133 Loss 0.028199434280395508\n",
            "Mini - Batch  134 Loss 0.035282790660858154\n",
            "Mini - Batch  135 Loss 0.032862842082977295\n",
            "Mini - Batch  136 Loss 0.046153903007507324\n",
            "Mini - Batch  137 Loss 0.02743840217590332\n",
            "Mini - Batch  138 Loss 0.0395129919052124\n",
            "Mini - Batch  139 Loss 0.02630060911178589\n",
            "Mini - Batch  140 Loss 0.046823084354400635\n",
            "Mini - Batch  141 Loss 0.034438133239746094\n",
            "Mini - Batch  142 Loss 0.019398808479309082\n",
            "Mini - Batch  143 Loss 0.03306978940963745\n",
            "Mini - Batch  144 Loss 0.025842368602752686\n",
            "Mini - Batch  145 Loss 0.024018049240112305\n",
            "Mini - Batch  146 Loss 0.03199267387390137\n",
            "Mini - Batch  147 Loss 0.02220827341079712\n",
            "Mini - Batch  148 Loss 0.03401756286621094\n",
            "Mini - Batch  149 Loss 0.02113419771194458\n",
            "Mini - Batch  150 Loss 0.03974968194961548\n",
            "Mini - Batch  151 Loss 0.03331881761550903\n",
            "Mini - Batch  152 Loss 0.01790517568588257\n",
            "Mini - Batch  153 Loss 0.03227496147155762\n",
            "Mini - Batch  154 Loss 0.05265200138092041\n",
            "Mini - Batch  155 Loss 0.01979440450668335\n",
            "Mini - Batch  156 Loss 0.047394633293151855\n",
            "Mini - Batch  157 Loss 0.029337823390960693\n",
            "Mini - Batch  158 Loss 0.02463233470916748\n",
            "Mini - Batch  159 Loss 0.030041813850402832\n",
            "Mini - Batch  160 Loss 0.022565901279449463\n",
            "Mini - Batch  161 Loss 0.019672095775604248\n",
            "Mini - Batch  162 Loss 0.04754692316055298\n",
            "Mini - Batch  163 Loss 0.021419763565063477\n",
            "Mini - Batch  164 Loss 0.023213446140289307\n",
            "Mini - Batch  165 Loss 0.028524816036224365\n",
            "Mini - Batch  166 Loss 0.04546689987182617\n",
            "\t Epoch Loss = [4] loss: 5.718\n",
            "Mini - Batch  0 Loss 0.02853238582611084\n",
            "Mini - Batch  1 Loss 0.018723726272583008\n",
            "Mini - Batch  2 Loss 0.020951151847839355\n",
            "Mini - Batch  3 Loss 0.026881694793701172\n",
            "Mini - Batch  4 Loss 0.024682343006134033\n",
            "Mini - Batch  5 Loss 0.020882070064544678\n",
            "Mini - Batch  6 Loss 0.020678281784057617\n",
            "Mini - Batch  7 Loss 0.045946359634399414\n",
            "Mini - Batch  8 Loss 0.024652838706970215\n",
            "Mini - Batch  9 Loss 0.0212976336479187\n",
            "Mini - Batch  10 Loss 0.02654862403869629\n",
            "Mini - Batch  11 Loss 0.05679821968078613\n",
            "Mini - Batch  12 Loss 0.03879356384277344\n",
            "Mini - Batch  13 Loss 0.02386927604675293\n",
            "Mini - Batch  14 Loss 0.03662163019180298\n",
            "Mini - Batch  15 Loss 0.019255220890045166\n",
            "Mini - Batch  16 Loss 0.028593361377716064\n",
            "Mini - Batch  17 Loss 0.03540712594985962\n",
            "Mini - Batch  18 Loss 0.03294867277145386\n",
            "Mini - Batch  19 Loss 0.031018316745758057\n",
            "Mini - Batch  20 Loss 0.022808074951171875\n",
            "Mini - Batch  21 Loss 0.04884696006774902\n",
            "Mini - Batch  22 Loss 0.02148115634918213\n",
            "Mini - Batch  23 Loss 0.046751201152801514\n",
            "Mini - Batch  24 Loss 0.07074636220932007\n",
            "Mini - Batch  25 Loss 0.01986825466156006\n",
            "Mini - Batch  26 Loss 0.04177159070968628\n",
            "Mini - Batch  27 Loss 0.07957971096038818\n",
            "Mini - Batch  28 Loss 0.026469826698303223\n",
            "Mini - Batch  29 Loss 0.060728609561920166\n",
            "Mini - Batch  30 Loss 0.03009343147277832\n",
            "Mini - Batch  31 Loss 0.0339508056640625\n",
            "Mini - Batch  32 Loss 0.02995055913925171\n",
            "Mini - Batch  33 Loss 0.030567944049835205\n",
            "Mini - Batch  34 Loss 0.04613214731216431\n",
            "Mini - Batch  35 Loss 0.030988097190856934\n",
            "Mini - Batch  36 Loss 0.030707716941833496\n",
            "Mini - Batch  37 Loss 0.01622694730758667\n",
            "Mini - Batch  38 Loss 0.01981830596923828\n",
            "Mini - Batch  39 Loss 0.054645419120788574\n",
            "Mini - Batch  40 Loss 0.018347978591918945\n",
            "Mini - Batch  41 Loss 0.04637545347213745\n",
            "Mini - Batch  42 Loss 0.058632850646972656\n",
            "Mini - Batch  43 Loss 0.019208848476409912\n",
            "Mini - Batch  44 Loss 0.01968371868133545\n",
            "Mini - Batch  45 Loss 0.023153066635131836\n",
            "Mini - Batch  46 Loss 0.027631282806396484\n",
            "Mini - Batch  47 Loss 0.025924086570739746\n",
            "Mini - Batch  48 Loss 0.017277836799621582\n",
            "Mini - Batch  49 Loss 0.021368682384490967\n",
            "Mini - Batch  50 Loss 0.02805948257446289\n",
            "Mini - Batch  51 Loss 0.02483808994293213\n",
            "Mini - Batch  52 Loss 0.03368312120437622\n",
            "Mini - Batch  53 Loss 0.017275452613830566\n",
            "Mini - Batch  54 Loss 0.021343886852264404\n",
            "Mini - Batch  55 Loss 0.04209643602371216\n",
            "Mini - Batch  56 Loss 0.019834518432617188\n",
            "Mini - Batch  57 Loss 0.04877549409866333\n",
            "Mini - Batch  58 Loss 0.03517085313796997\n",
            "Mini - Batch  59 Loss 0.028413712978363037\n",
            "Mini - Batch  60 Loss 0.127924382686615\n",
            "Mini - Batch  61 Loss 0.03077995777130127\n",
            "Mini - Batch  62 Loss 0.025179922580718994\n",
            "Mini - Batch  63 Loss 0.02210289239883423\n",
            "Mini - Batch  64 Loss 0.024984896183013916\n",
            "Mini - Batch  65 Loss 0.014925539493560791\n",
            "Mini - Batch  66 Loss 0.032954394817352295\n",
            "Mini - Batch  67 Loss 0.022723376750946045\n",
            "Mini - Batch  68 Loss 0.03580659627914429\n",
            "Mini - Batch  69 Loss 0.037289559841156006\n",
            "Mini - Batch  70 Loss 0.03291422128677368\n",
            "Mini - Batch  71 Loss 0.040466129779815674\n",
            "Mini - Batch  72 Loss 0.028604745864868164\n",
            "Mini - Batch  73 Loss 0.014901876449584961\n",
            "Mini - Batch  74 Loss 0.12389540672302246\n",
            "Mini - Batch  75 Loss 0.01714491844177246\n",
            "Mini - Batch  76 Loss 0.045291244983673096\n",
            "Mini - Batch  77 Loss 0.02289944887161255\n",
            "Mini - Batch  78 Loss 0.027017295360565186\n",
            "Mini - Batch  79 Loss 0.030933737754821777\n",
            "Mini - Batch  80 Loss 0.038508713245391846\n",
            "Mini - Batch  81 Loss 0.01968860626220703\n",
            "Mini - Batch  82 Loss 0.03305554389953613\n",
            "Mini - Batch  83 Loss 0.02883976697921753\n",
            "Mini - Batch  84 Loss 0.026844918727874756\n",
            "Mini - Batch  85 Loss 0.02484118938446045\n",
            "Mini - Batch  86 Loss 0.027736961841583252\n",
            "Mini - Batch  87 Loss 0.024063050746917725\n",
            "Mini - Batch  88 Loss 0.057098209857940674\n",
            "Mini - Batch  89 Loss 0.030677735805511475\n",
            "Mini - Batch  90 Loss 0.02658402919769287\n",
            "Mini - Batch  91 Loss 0.03284943103790283\n",
            "Mini - Batch  92 Loss 0.026851177215576172\n",
            "Mini - Batch  93 Loss 0.01819819211959839\n",
            "Mini - Batch  94 Loss 0.024692893028259277\n",
            "Mini - Batch  95 Loss 0.03682076930999756\n",
            "Mini - Batch  96 Loss 0.017521381378173828\n",
            "Mini - Batch  97 Loss 0.020724475383758545\n",
            "Mini - Batch  98 Loss 0.017647087574005127\n",
            "Mini - Batch  99 Loss 0.028570473194122314\n",
            "Mini - Batch  100 Loss 0.021646857261657715\n",
            "Mini - Batch  101 Loss 0.026827871799468994\n",
            "Mini - Batch  102 Loss 0.03488659858703613\n",
            "Mini - Batch  103 Loss 0.024304211139678955\n",
            "Mini - Batch  104 Loss 0.020708024501800537\n",
            "Mini - Batch  105 Loss 0.02240169048309326\n",
            "Mini - Batch  106 Loss 0.027983784675598145\n",
            "Mini - Batch  107 Loss 0.021710216999053955\n",
            "Mini - Batch  108 Loss 0.024220585823059082\n",
            "Mini - Batch  109 Loss 0.021539747714996338\n",
            "Mini - Batch  110 Loss 0.03674900531768799\n",
            "Mini - Batch  111 Loss 0.02965545654296875\n",
            "Mini - Batch  112 Loss 0.027790307998657227\n",
            "Mini - Batch  113 Loss 0.06560713052749634\n",
            "Mini - Batch  114 Loss 0.05584239959716797\n",
            "Mini - Batch  115 Loss 0.04639488458633423\n",
            "Mini - Batch  116 Loss 0.02978616952896118\n",
            "Mini - Batch  117 Loss 0.05088448524475098\n",
            "Mini - Batch  118 Loss 0.02517145872116089\n",
            "Mini - Batch  119 Loss 0.016473889350891113\n",
            "Mini - Batch  120 Loss 0.026423215866088867\n",
            "Mini - Batch  121 Loss 0.03202509880065918\n",
            "Mini - Batch  122 Loss 0.02178138494491577\n",
            "Mini - Batch  123 Loss 0.0325811505317688\n",
            "Mini - Batch  124 Loss 0.05821281671524048\n",
            "Mini - Batch  125 Loss 0.022953927516937256\n",
            "Mini - Batch  126 Loss 0.12404757738113403\n",
            "Mini - Batch  127 Loss 0.01832979917526245\n",
            "Mini - Batch  128 Loss 0.031520068645477295\n",
            "Mini - Batch  129 Loss 0.019447743892669678\n",
            "Mini - Batch  130 Loss 0.022780954837799072\n",
            "Mini - Batch  131 Loss 0.034963130950927734\n",
            "Mini - Batch  132 Loss 0.03798389434814453\n",
            "Mini - Batch  133 Loss 0.023900389671325684\n",
            "Mini - Batch  134 Loss 0.01918649673461914\n",
            "Mini - Batch  135 Loss 0.029344618320465088\n",
            "Mini - Batch  136 Loss 0.049226462841033936\n",
            "Mini - Batch  137 Loss 0.026253163814544678\n",
            "Mini - Batch  138 Loss 0.030546605587005615\n",
            "Mini - Batch  139 Loss 0.05289393663406372\n",
            "Mini - Batch  140 Loss 0.02139812707901001\n",
            "Mini - Batch  141 Loss 0.02195882797241211\n",
            "Mini - Batch  142 Loss 0.02963024377822876\n",
            "Mini - Batch  143 Loss 0.019335269927978516\n",
            "Mini - Batch  144 Loss 0.06580245494842529\n",
            "Mini - Batch  145 Loss 0.017373204231262207\n",
            "Mini - Batch  146 Loss 0.02945554256439209\n",
            "Mini - Batch  147 Loss 0.1700897216796875\n",
            "Mini - Batch  148 Loss 0.030647099018096924\n",
            "Mini - Batch  149 Loss 0.017453312873840332\n",
            "Mini - Batch  150 Loss 0.02105814218521118\n",
            "Mini - Batch  151 Loss 0.019567906856536865\n",
            "Mini - Batch  152 Loss 0.027875661849975586\n",
            "Mini - Batch  153 Loss 0.02250009775161743\n",
            "Mini - Batch  154 Loss 0.031225264072418213\n",
            "Mini - Batch  155 Loss 0.01587212085723877\n",
            "Mini - Batch  156 Loss 0.03915750980377197\n",
            "Mini - Batch  157 Loss 0.04065549373626709\n",
            "Mini - Batch  158 Loss 0.01967853307723999\n",
            "Mini - Batch  159 Loss 0.018632948398590088\n",
            "Mini - Batch  160 Loss 0.01755833625793457\n",
            "Mini - Batch  161 Loss 0.05467146635055542\n",
            "Mini - Batch  162 Loss 0.025167644023895264\n",
            "Mini - Batch  163 Loss 0.022080063819885254\n",
            "Mini - Batch  164 Loss 0.026774466037750244\n",
            "Mini - Batch  165 Loss 0.027138352394104004\n",
            "Mini - Batch  166 Loss 0.023679375648498535\n",
            "\t Epoch Loss = [5] loss: 5.464\n",
            "Mini - Batch  0 Loss 0.026655733585357666\n",
            "Mini - Batch  1 Loss 0.020817875862121582\n",
            "Mini - Batch  2 Loss 0.02792346477508545\n",
            "Mini - Batch  3 Loss 0.019539952278137207\n",
            "Mini - Batch  4 Loss 0.03408414125442505\n",
            "Mini - Batch  5 Loss 0.026659071445465088\n",
            "Mini - Batch  6 Loss 0.023890972137451172\n",
            "Mini - Batch  7 Loss 0.01958918571472168\n",
            "Mini - Batch  8 Loss 0.024688720703125\n",
            "Mini - Batch  9 Loss 0.020405828952789307\n",
            "Mini - Batch  10 Loss 0.02240169048309326\n",
            "Mini - Batch  11 Loss 0.01905965805053711\n",
            "Mini - Batch  12 Loss 0.04954063892364502\n",
            "Mini - Batch  13 Loss 0.02603834867477417\n",
            "Mini - Batch  14 Loss 0.12082868814468384\n",
            "Mini - Batch  15 Loss 0.02985978126525879\n",
            "Mini - Batch  16 Loss 0.03002750873565674\n",
            "Mini - Batch  17 Loss 0.02924489974975586\n",
            "Mini - Batch  18 Loss 0.02537834644317627\n",
            "Mini - Batch  19 Loss 0.03100132942199707\n",
            "Mini - Batch  20 Loss 0.020641863346099854\n",
            "Mini - Batch  21 Loss 0.014466643333435059\n",
            "Mini - Batch  22 Loss 0.03330320119857788\n",
            "Mini - Batch  23 Loss 0.019173026084899902\n",
            "Mini - Batch  24 Loss 0.01765376329421997\n",
            "Mini - Batch  25 Loss 0.022260308265686035\n",
            "Mini - Batch  26 Loss 0.024343132972717285\n",
            "Mini - Batch  27 Loss 0.017312288284301758\n",
            "Mini - Batch  28 Loss 0.0233725905418396\n",
            "Mini - Batch  29 Loss 0.01615363359451294\n",
            "Mini - Batch  30 Loss 0.019172251224517822\n",
            "Mini - Batch  31 Loss 0.016527235507965088\n",
            "Mini - Batch  32 Loss 0.02953040599822998\n",
            "Mini - Batch  33 Loss 0.018501222133636475\n",
            "Mini - Batch  34 Loss 0.044290125370025635\n",
            "Mini - Batch  35 Loss 0.032710492610931396\n",
            "Mini - Batch  36 Loss 0.01706671714782715\n",
            "Mini - Batch  37 Loss 0.01838397979736328\n",
            "Mini - Batch  38 Loss 0.02988588809967041\n",
            "Mini - Batch  39 Loss 0.035377681255340576\n",
            "Mini - Batch  40 Loss 0.026104986667633057\n",
            "Mini - Batch  41 Loss 0.025257110595703125\n",
            "Mini - Batch  42 Loss 0.03389865159988403\n",
            "Mini - Batch  43 Loss 0.020281612873077393\n",
            "Mini - Batch  44 Loss 0.022593915462493896\n",
            "Mini - Batch  45 Loss 0.025154292583465576\n",
            "Mini - Batch  46 Loss 0.02440851926803589\n",
            "Mini - Batch  47 Loss 0.025496184825897217\n",
            "Mini - Batch  48 Loss 0.02075815200805664\n",
            "Mini - Batch  49 Loss 0.01820129156112671\n",
            "Mini - Batch  50 Loss 0.02647644281387329\n",
            "Mini - Batch  51 Loss 0.0174863338470459\n",
            "Mini - Batch  52 Loss 0.040122807025909424\n",
            "Mini - Batch  53 Loss 0.022775709629058838\n",
            "Mini - Batch  54 Loss 0.01741635799407959\n",
            "Mini - Batch  55 Loss 0.020571231842041016\n",
            "Mini - Batch  56 Loss 0.05950117111206055\n",
            "Mini - Batch  57 Loss 0.0519985556602478\n",
            "Mini - Batch  58 Loss 0.041137635707855225\n",
            "Mini - Batch  59 Loss 0.024075210094451904\n",
            "Mini - Batch  60 Loss 0.03297227621078491\n",
            "Mini - Batch  61 Loss 0.019314944744110107\n",
            "Mini - Batch  62 Loss 0.016505897045135498\n",
            "Mini - Batch  63 Loss 0.01957803964614868\n",
            "Mini - Batch  64 Loss 0.036523640155792236\n",
            "Mini - Batch  65 Loss 0.05117297172546387\n",
            "Mini - Batch  66 Loss 0.0265730619430542\n",
            "Mini - Batch  67 Loss 0.017096281051635742\n",
            "Mini - Batch  68 Loss 0.024226844310760498\n",
            "Mini - Batch  69 Loss 0.03791677951812744\n",
            "Mini - Batch  70 Loss 0.01515573263168335\n",
            "Mini - Batch  71 Loss 0.02704864740371704\n",
            "Mini - Batch  72 Loss 0.09130561351776123\n",
            "Mini - Batch  73 Loss 0.03854727745056152\n",
            "Mini - Batch  74 Loss 0.033129751682281494\n",
            "Mini - Batch  75 Loss 0.04823487997055054\n",
            "Mini - Batch  76 Loss 0.020045459270477295\n",
            "Mini - Batch  77 Loss 0.0450400710105896\n",
            "Mini - Batch  78 Loss 0.020543575286865234\n",
            "Mini - Batch  79 Loss 0.07015877962112427\n",
            "Mini - Batch  80 Loss 0.021883487701416016\n",
            "Mini - Batch  81 Loss 0.021546006202697754\n",
            "Mini - Batch  82 Loss 0.028401076793670654\n",
            "Mini - Batch  83 Loss 0.035713911056518555\n",
            "Mini - Batch  84 Loss 0.030565500259399414\n",
            "Mini - Batch  85 Loss 0.02959585189819336\n",
            "Mini - Batch  86 Loss 0.02997565269470215\n",
            "Mini - Batch  87 Loss 0.02241307497024536\n",
            "Mini - Batch  88 Loss 0.03035604953765869\n",
            "Mini - Batch  89 Loss 0.07705610990524292\n",
            "Mini - Batch  90 Loss 0.024289965629577637\n",
            "Mini - Batch  91 Loss 0.05645287036895752\n",
            "Mini - Batch  92 Loss 0.019250571727752686\n",
            "Mini - Batch  93 Loss 0.01768893003463745\n",
            "Mini - Batch  94 Loss 0.05246204137802124\n",
            "Mini - Batch  95 Loss 0.021525025367736816\n",
            "Mini - Batch  96 Loss 0.22321546077728271\n",
            "Mini - Batch  97 Loss 0.030573487281799316\n",
            "Mini - Batch  98 Loss 0.02378004789352417\n",
            "Mini - Batch  99 Loss 0.04180413484573364\n",
            "Mini - Batch  100 Loss 0.01993614435195923\n",
            "Mini - Batch  101 Loss 0.02176600694656372\n",
            "Mini - Batch  102 Loss 0.03820532560348511\n",
            "Mini - Batch  103 Loss 0.02792048454284668\n",
            "Mini - Batch  104 Loss 0.02087104320526123\n",
            "Mini - Batch  105 Loss 0.02770555019378662\n",
            "Mini - Batch  106 Loss 0.062065184116363525\n",
            "Mini - Batch  107 Loss 0.019839107990264893\n",
            "Mini - Batch  108 Loss 0.17597317695617676\n",
            "Mini - Batch  109 Loss 0.05444025993347168\n",
            "Mini - Batch  110 Loss 0.03506731986999512\n",
            "Mini - Batch  111 Loss 0.025614619255065918\n",
            "Mini - Batch  112 Loss 0.022521138191223145\n",
            "Mini - Batch  113 Loss 0.06288892030715942\n",
            "Mini - Batch  114 Loss 0.03336220979690552\n",
            "Mini - Batch  115 Loss 0.040154874324798584\n",
            "Mini - Batch  116 Loss 0.0425792932510376\n",
            "Mini - Batch  117 Loss 0.026809096336364746\n",
            "Mini - Batch  118 Loss 0.02547752857208252\n",
            "Mini - Batch  119 Loss 0.03635966777801514\n",
            "Mini - Batch  120 Loss 0.034714698791503906\n",
            "Mini - Batch  121 Loss 0.019681155681610107\n",
            "Mini - Batch  122 Loss 0.027365028858184814\n",
            "Mini - Batch  123 Loss 0.023615121841430664\n",
            "Mini - Batch  124 Loss 0.02521669864654541\n",
            "Mini - Batch  125 Loss 0.031429409980773926\n",
            "Mini - Batch  126 Loss 0.023842573165893555\n",
            "Mini - Batch  127 Loss 0.01892179250717163\n",
            "Mini - Batch  128 Loss 0.01586127281188965\n",
            "Mini - Batch  129 Loss 0.024704575538635254\n",
            "Mini - Batch  130 Loss 0.019859790802001953\n",
            "Mini - Batch  131 Loss 0.026053547859191895\n",
            "Mini - Batch  132 Loss 0.020725667476654053\n",
            "Mini - Batch  133 Loss 0.04089123010635376\n",
            "Mini - Batch  134 Loss 0.024679183959960938\n",
            "Mini - Batch  135 Loss 0.02455967664718628\n",
            "Mini - Batch  136 Loss 0.016766250133514404\n",
            "Mini - Batch  137 Loss 0.017362773418426514\n",
            "Mini - Batch  138 Loss 0.07885462045669556\n",
            "Mini - Batch  139 Loss 0.03131359815597534\n",
            "Mini - Batch  140 Loss 0.017929017543792725\n",
            "Mini - Batch  141 Loss 0.029359638690948486\n",
            "Mini - Batch  142 Loss 0.017851054668426514\n",
            "Mini - Batch  143 Loss 0.026285886764526367\n",
            "Mini - Batch  144 Loss 0.02683490514755249\n",
            "Mini - Batch  145 Loss 0.03140723705291748\n",
            "Mini - Batch  146 Loss 0.024421393871307373\n",
            "Mini - Batch  147 Loss 0.02549201250076294\n",
            "Mini - Batch  148 Loss 0.0218009352684021\n",
            "Mini - Batch  149 Loss 0.039764583110809326\n",
            "Mini - Batch  150 Loss 0.01976644992828369\n",
            "Mini - Batch  151 Loss 0.02207082509994507\n",
            "Mini - Batch  152 Loss 0.01952505111694336\n",
            "Mini - Batch  153 Loss 0.048725783824920654\n",
            "Mini - Batch  154 Loss 0.0209311842918396\n",
            "Mini - Batch  155 Loss 0.01860600709915161\n",
            "Mini - Batch  156 Loss 0.029367804527282715\n",
            "Mini - Batch  157 Loss 0.02080601453781128\n",
            "Mini - Batch  158 Loss 0.02839040756225586\n",
            "Mini - Batch  159 Loss 0.02327185869216919\n",
            "Mini - Batch  160 Loss 0.022608697414398193\n",
            "Mini - Batch  161 Loss 0.021492063999176025\n",
            "Mini - Batch  162 Loss 0.021835386753082275\n",
            "Mini - Batch  163 Loss 0.014961600303649902\n",
            "Mini - Batch  164 Loss 0.03379392623901367\n",
            "Mini - Batch  165 Loss 0.016877293586730957\n",
            "Mini - Batch  166 Loss 0.018844783306121826\n",
            "\t Epoch Loss = [6] loss: 5.220\n",
            "Mini - Batch  0 Loss 0.030836164951324463\n",
            "Mini - Batch  1 Loss 0.02211099863052368\n",
            "Mini - Batch  2 Loss 0.030014872550964355\n",
            "Mini - Batch  3 Loss 0.02420741319656372\n",
            "Mini - Batch  4 Loss 0.1799154281616211\n",
            "Mini - Batch  5 Loss 0.019897282123565674\n",
            "Mini - Batch  6 Loss 0.015175282955169678\n",
            "Mini - Batch  7 Loss 0.027257204055786133\n",
            "Mini - Batch  8 Loss 0.016073405742645264\n",
            "Mini - Batch  9 Loss 0.022569715976715088\n",
            "Mini - Batch  10 Loss 0.01721048355102539\n",
            "Mini - Batch  11 Loss 0.02867656946182251\n",
            "Mini - Batch  12 Loss 0.02014535665512085\n",
            "Mini - Batch  13 Loss 0.03324037790298462\n",
            "Mini - Batch  14 Loss 0.02783787250518799\n",
            "Mini - Batch  15 Loss 0.025310516357421875\n",
            "Mini - Batch  16 Loss 0.02200371026992798\n",
            "Mini - Batch  17 Loss 0.02260226011276245\n",
            "Mini - Batch  18 Loss 0.022556304931640625\n",
            "Mini - Batch  19 Loss 0.016556859016418457\n",
            "Mini - Batch  20 Loss 0.029276132583618164\n",
            "Mini - Batch  21 Loss 0.01894199848175049\n",
            "Mini - Batch  22 Loss 0.025301039218902588\n",
            "Mini - Batch  23 Loss 0.029291212558746338\n",
            "Mini - Batch  24 Loss 0.03234446048736572\n",
            "Mini - Batch  25 Loss 0.021237313747406006\n",
            "Mini - Batch  26 Loss 0.020318806171417236\n",
            "Mini - Batch  27 Loss 0.02118462324142456\n",
            "Mini - Batch  28 Loss 0.020767033100128174\n",
            "Mini - Batch  29 Loss 0.028710246086120605\n",
            "Mini - Batch  30 Loss 0.028399229049682617\n",
            "Mini - Batch  31 Loss 0.01997530460357666\n",
            "Mini - Batch  32 Loss 0.02056145668029785\n",
            "Mini - Batch  33 Loss 0.03473007678985596\n",
            "Mini - Batch  34 Loss 0.015823423862457275\n",
            "Mini - Batch  35 Loss 0.020454108715057373\n",
            "Mini - Batch  36 Loss 0.021207809448242188\n",
            "Mini - Batch  37 Loss 0.031886518001556396\n",
            "Mini - Batch  38 Loss 0.03069019317626953\n",
            "Mini - Batch  39 Loss 0.02176487445831299\n",
            "Mini - Batch  40 Loss 0.03319382667541504\n",
            "Mini - Batch  41 Loss 0.021296977996826172\n",
            "Mini - Batch  42 Loss 0.01748478412628174\n",
            "Mini - Batch  43 Loss 0.016269266605377197\n",
            "Mini - Batch  44 Loss 0.03204578161239624\n",
            "Mini - Batch  45 Loss 0.024710416793823242\n",
            "Mini - Batch  46 Loss 0.03143584728240967\n",
            "Mini - Batch  47 Loss 0.017378926277160645\n",
            "Mini - Batch  48 Loss 0.021482110023498535\n",
            "Mini - Batch  49 Loss 0.026651382446289062\n",
            "Mini - Batch  50 Loss 0.016247689723968506\n",
            "Mini - Batch  51 Loss 0.13441497087478638\n",
            "Mini - Batch  52 Loss 0.021940529346466064\n",
            "Mini - Batch  53 Loss 0.026470303535461426\n",
            "Mini - Batch  54 Loss 0.0382533073425293\n",
            "Mini - Batch  55 Loss 0.04097700119018555\n",
            "Mini - Batch  56 Loss 0.022424817085266113\n",
            "Mini - Batch  57 Loss 0.026877224445343018\n",
            "Mini - Batch  58 Loss 0.04052978754043579\n",
            "Mini - Batch  59 Loss 0.032849013805389404\n",
            "Mini - Batch  60 Loss 0.0325469970703125\n",
            "Mini - Batch  61 Loss 0.020673394203186035\n",
            "Mini - Batch  62 Loss 0.036807477474212646\n",
            "Mini - Batch  63 Loss 0.023349404335021973\n",
            "Mini - Batch  64 Loss 0.025349199771881104\n",
            "Mini - Batch  65 Loss 0.02695995569229126\n",
            "Mini - Batch  66 Loss 0.029755592346191406\n",
            "Mini - Batch  67 Loss 0.2356940507888794\n",
            "Mini - Batch  68 Loss 0.04114866256713867\n",
            "Mini - Batch  69 Loss 0.03290671110153198\n",
            "Mini - Batch  70 Loss 0.1240798830986023\n",
            "Mini - Batch  71 Loss 0.03195393085479736\n",
            "Mini - Batch  72 Loss 0.048961758613586426\n",
            "Mini - Batch  73 Loss 0.020866572856903076\n",
            "Mini - Batch  74 Loss 0.024758994579315186\n",
            "Mini - Batch  75 Loss 0.026722252368927002\n",
            "Mini - Batch  76 Loss 0.050872087478637695\n",
            "Mini - Batch  77 Loss 0.015582919120788574\n",
            "Mini - Batch  78 Loss 0.02322608232498169\n",
            "Mini - Batch  79 Loss 0.022504806518554688\n",
            "Mini - Batch  80 Loss 0.01873689889907837\n",
            "Mini - Batch  81 Loss 0.036733806133270264\n",
            "Mini - Batch  82 Loss 0.041976213455200195\n",
            "Mini - Batch  83 Loss 0.01905536651611328\n",
            "Mini - Batch  84 Loss 0.03589588403701782\n",
            "Mini - Batch  85 Loss 0.013866066932678223\n",
            "Mini - Batch  86 Loss 0.019617021083831787\n",
            "Mini - Batch  87 Loss 0.1291101574897766\n",
            "Mini - Batch  88 Loss 0.024384379386901855\n",
            "Mini - Batch  89 Loss 0.03329497575759888\n",
            "Mini - Batch  90 Loss 0.024922549724578857\n",
            "Mini - Batch  91 Loss 0.02244889736175537\n",
            "Mini - Batch  92 Loss 0.027173101902008057\n",
            "Mini - Batch  93 Loss 0.017899751663208008\n",
            "Mini - Batch  94 Loss 0.030143260955810547\n",
            "Mini - Batch  95 Loss 0.018780827522277832\n",
            "Mini - Batch  96 Loss 0.03401070833206177\n",
            "Mini - Batch  97 Loss 0.03057265281677246\n",
            "Mini - Batch  98 Loss 0.043663203716278076\n",
            "Mini - Batch  99 Loss 0.04107236862182617\n",
            "Mini - Batch  100 Loss 0.02358412742614746\n",
            "Mini - Batch  101 Loss 0.01849132776260376\n",
            "Mini - Batch  102 Loss 0.03188520669937134\n",
            "Mini - Batch  103 Loss 0.016092419624328613\n",
            "Mini - Batch  104 Loss 0.030223309993743896\n",
            "Mini - Batch  105 Loss 0.026470482349395752\n",
            "Mini - Batch  106 Loss 0.06003361940383911\n",
            "Mini - Batch  107 Loss 0.046584129333496094\n",
            "Mini - Batch  108 Loss 0.029493391513824463\n",
            "Mini - Batch  109 Loss 0.01676279306411743\n",
            "Mini - Batch  110 Loss 0.021796464920043945\n",
            "Mini - Batch  111 Loss 0.03248518705368042\n",
            "Mini - Batch  112 Loss 0.03787201642990112\n",
            "Mini - Batch  113 Loss 0.024035394191741943\n",
            "Mini - Batch  114 Loss 0.029357552528381348\n",
            "Mini - Batch  115 Loss 0.048377275466918945\n",
            "Mini - Batch  116 Loss 0.018321633338928223\n",
            "Mini - Batch  117 Loss 0.032432734966278076\n",
            "Mini - Batch  118 Loss 0.017272114753723145\n",
            "Mini - Batch  119 Loss 0.026607751846313477\n",
            "Mini - Batch  120 Loss 0.024339675903320312\n",
            "Mini - Batch  121 Loss 0.03876274824142456\n",
            "Mini - Batch  122 Loss 0.044968366622924805\n",
            "Mini - Batch  123 Loss 0.03352534770965576\n",
            "Mini - Batch  124 Loss 0.039867401123046875\n",
            "Mini - Batch  125 Loss 0.018574237823486328\n",
            "Mini - Batch  126 Loss 0.027452826499938965\n",
            "Mini - Batch  127 Loss 0.0219193696975708\n",
            "Mini - Batch  128 Loss 0.016605079174041748\n",
            "Mini - Batch  129 Loss 0.026846587657928467\n",
            "Mini - Batch  130 Loss 0.02311861515045166\n",
            "Mini - Batch  131 Loss 0.021379053592681885\n",
            "Mini - Batch  132 Loss 0.027049481868743896\n",
            "Mini - Batch  133 Loss 0.018607676029205322\n",
            "Mini - Batch  134 Loss 0.022159218788146973\n",
            "Mini - Batch  135 Loss 0.019050300121307373\n",
            "Mini - Batch  136 Loss 0.040909767150878906\n",
            "Mini - Batch  137 Loss 0.0249100923538208\n",
            "Mini - Batch  138 Loss 0.019475996494293213\n",
            "Mini - Batch  139 Loss 0.0405879020690918\n",
            "Mini - Batch  140 Loss 0.03233915567398071\n",
            "Mini - Batch  141 Loss 0.01556020975112915\n",
            "Mini - Batch  142 Loss 0.03564786911010742\n",
            "Mini - Batch  143 Loss 0.016585469245910645\n",
            "Mini - Batch  144 Loss 0.030753731727600098\n",
            "Mini - Batch  145 Loss 0.019502460956573486\n",
            "Mini - Batch  146 Loss 0.015719592571258545\n",
            "Mini - Batch  147 Loss 0.020409345626831055\n",
            "Mini - Batch  148 Loss 0.01602327823638916\n",
            "Mini - Batch  149 Loss 0.02487623691558838\n",
            "Mini - Batch  150 Loss 0.02785438299179077\n",
            "Mini - Batch  151 Loss 0.032892823219299316\n",
            "Mini - Batch  152 Loss 0.032579243183135986\n",
            "Mini - Batch  153 Loss 0.02796041965484619\n",
            "Mini - Batch  154 Loss 0.05592089891433716\n",
            "Mini - Batch  155 Loss 0.029025733470916748\n",
            "Mini - Batch  156 Loss 0.023063242435455322\n",
            "Mini - Batch  157 Loss 0.037891387939453125\n",
            "Mini - Batch  158 Loss 0.022523343563079834\n",
            "Mini - Batch  159 Loss 0.017376959323883057\n",
            "Mini - Batch  160 Loss 0.029442131519317627\n",
            "Mini - Batch  161 Loss 0.0308077335357666\n",
            "Mini - Batch  162 Loss 0.021659672260284424\n",
            "Mini - Batch  163 Loss 0.02414572238922119\n",
            "Mini - Batch  164 Loss 0.02873176336288452\n",
            "Mini - Batch  165 Loss 0.021127820014953613\n",
            "Mini - Batch  166 Loss 0.01799917221069336\n",
            "\t Epoch Loss = [7] loss: 5.165\n",
            "Mini - Batch  0 Loss 0.018728792667388916\n",
            "Mini - Batch  1 Loss 0.029206812381744385\n",
            "Mini - Batch  2 Loss 0.02274608612060547\n",
            "Mini - Batch  3 Loss 0.01718735694885254\n",
            "Mini - Batch  4 Loss 0.1086266040802002\n",
            "Mini - Batch  5 Loss 0.015319585800170898\n",
            "Mini - Batch  6 Loss 0.015861213207244873\n",
            "Mini - Batch  7 Loss 0.014758944511413574\n",
            "Mini - Batch  8 Loss 0.016825199127197266\n",
            "Mini - Batch  9 Loss 0.025357306003570557\n",
            "Mini - Batch  10 Loss 0.020161092281341553\n",
            "Mini - Batch  11 Loss 0.030228614807128906\n",
            "Mini - Batch  12 Loss 0.020978331565856934\n",
            "Mini - Batch  13 Loss 0.04445147514343262\n",
            "Mini - Batch  14 Loss 0.03291451930999756\n",
            "Mini - Batch  15 Loss 0.0399705171585083\n",
            "Mini - Batch  16 Loss 0.017410695552825928\n",
            "Mini - Batch  17 Loss 0.034429311752319336\n",
            "Mini - Batch  18 Loss 0.014926910400390625\n",
            "Mini - Batch  19 Loss 0.018699467182159424\n",
            "Mini - Batch  20 Loss 0.02843838930130005\n",
            "Mini - Batch  21 Loss 0.023828327655792236\n",
            "Mini - Batch  22 Loss 0.020774126052856445\n",
            "Mini - Batch  23 Loss 0.029994845390319824\n",
            "Mini - Batch  24 Loss 0.02199167013168335\n",
            "Mini - Batch  25 Loss 0.02268373966217041\n",
            "Mini - Batch  26 Loss 0.02102905511856079\n",
            "Mini - Batch  27 Loss 0.019835948944091797\n",
            "Mini - Batch  28 Loss 0.01521599292755127\n",
            "Mini - Batch  29 Loss 0.13329017162322998\n",
            "Mini - Batch  30 Loss 0.026339590549468994\n",
            "Mini - Batch  31 Loss 0.025005996227264404\n",
            "Mini - Batch  32 Loss 0.05728954076766968\n",
            "Mini - Batch  33 Loss 0.029313266277313232\n",
            "Mini - Batch  34 Loss 0.02786654233932495\n",
            "Mini - Batch  35 Loss 0.03708285093307495\n",
            "Mini - Batch  36 Loss 0.03238487243652344\n",
            "Mini - Batch  37 Loss 0.02861201763153076\n",
            "Mini - Batch  38 Loss 0.029568612575531006\n",
            "Mini - Batch  39 Loss 0.027477562427520752\n",
            "Mini - Batch  40 Loss 0.03208798170089722\n",
            "Mini - Batch  41 Loss 0.025137126445770264\n",
            "Mini - Batch  42 Loss 0.03146934509277344\n",
            "Mini - Batch  43 Loss 0.02950519323348999\n",
            "Mini - Batch  44 Loss 0.022227227687835693\n",
            "Mini - Batch  45 Loss 0.0203629732131958\n",
            "Mini - Batch  46 Loss 0.027536213397979736\n",
            "Mini - Batch  47 Loss 0.02414029836654663\n",
            "Mini - Batch  48 Loss 0.01460409164428711\n",
            "Mini - Batch  49 Loss 0.0357549786567688\n",
            "Mini - Batch  50 Loss 0.027601957321166992\n",
            "Mini - Batch  51 Loss 0.027366995811462402\n",
            "Mini - Batch  52 Loss 0.02204054594039917\n",
            "Mini - Batch  53 Loss 0.10037869215011597\n",
            "Mini - Batch  54 Loss 0.018363773822784424\n",
            "Mini - Batch  55 Loss 0.0312003493309021\n",
            "Mini - Batch  56 Loss 0.0223844051361084\n",
            "Mini - Batch  57 Loss 0.01958441734313965\n",
            "Mini - Batch  58 Loss 0.028536617755889893\n",
            "Mini - Batch  59 Loss 0.02795499563217163\n",
            "Mini - Batch  60 Loss 0.018461287021636963\n",
            "Mini - Batch  61 Loss 0.022162556648254395\n",
            "Mini - Batch  62 Loss 0.022506415843963623\n",
            "Mini - Batch  63 Loss 0.017418265342712402\n",
            "Mini - Batch  64 Loss 0.021769940853118896\n",
            "Mini - Batch  65 Loss 0.020641326904296875\n",
            "Mini - Batch  66 Loss 0.022512614727020264\n",
            "Mini - Batch  67 Loss 0.035324692726135254\n",
            "Mini - Batch  68 Loss 0.024214327335357666\n",
            "Mini - Batch  69 Loss 0.07210427522659302\n",
            "Mini - Batch  70 Loss 0.03910326957702637\n",
            "Mini - Batch  71 Loss 0.040495216846466064\n",
            "Mini - Batch  72 Loss 0.01996588706970215\n",
            "Mini - Batch  73 Loss 0.02915114164352417\n",
            "Mini - Batch  74 Loss 0.02227616310119629\n",
            "Mini - Batch  75 Loss 0.025748074054718018\n",
            "Mini - Batch  76 Loss 0.03513824939727783\n",
            "Mini - Batch  77 Loss 0.05496871471405029\n",
            "Mini - Batch  78 Loss 0.020353376865386963\n",
            "Mini - Batch  79 Loss 0.029547691345214844\n",
            "Mini - Batch  80 Loss 0.07076781988143921\n",
            "Mini - Batch  81 Loss 0.027000069618225098\n",
            "Mini - Batch  82 Loss 0.03796952962875366\n",
            "Mini - Batch  83 Loss 0.029309988021850586\n",
            "Mini - Batch  84 Loss 0.025933921337127686\n",
            "Mini - Batch  85 Loss 0.02444601058959961\n",
            "Mini - Batch  86 Loss 0.021345853805541992\n",
            "Mini - Batch  87 Loss 0.020186305046081543\n",
            "Mini - Batch  88 Loss 0.02379375696182251\n",
            "Mini - Batch  89 Loss 0.021264493465423584\n",
            "Mini - Batch  90 Loss 0.029263079166412354\n",
            "Mini - Batch  91 Loss 0.013987064361572266\n",
            "Mini - Batch  92 Loss 0.021741211414337158\n",
            "Mini - Batch  93 Loss 0.02854865789413452\n",
            "Mini - Batch  94 Loss 0.025773823261260986\n",
            "Mini - Batch  95 Loss 0.019104957580566406\n",
            "Mini - Batch  96 Loss 0.018621206283569336\n",
            "Mini - Batch  97 Loss 0.016026437282562256\n",
            "Mini - Batch  98 Loss 0.027362823486328125\n",
            "Mini - Batch  99 Loss 0.032688260078430176\n",
            "Mini - Batch  100 Loss 0.027632296085357666\n",
            "Mini - Batch  101 Loss 0.017381787300109863\n",
            "Mini - Batch  102 Loss 0.03422123193740845\n",
            "Mini - Batch  103 Loss 0.021655261516571045\n",
            "Mini - Batch  104 Loss 0.024398624897003174\n",
            "Mini - Batch  105 Loss 0.02722644805908203\n",
            "Mini - Batch  106 Loss 0.02203470468521118\n",
            "Mini - Batch  107 Loss 0.03256022930145264\n",
            "Mini - Batch  108 Loss 0.021443307399749756\n",
            "Mini - Batch  109 Loss 0.01626110076904297\n",
            "Mini - Batch  110 Loss 0.024385511875152588\n",
            "Mini - Batch  111 Loss 0.021575212478637695\n",
            "Mini - Batch  112 Loss 0.027873098850250244\n",
            "Mini - Batch  113 Loss 0.03138655424118042\n",
            "Mini - Batch  114 Loss 0.021176576614379883\n",
            "Mini - Batch  115 Loss 0.06652891635894775\n",
            "Mini - Batch  116 Loss 0.017946064472198486\n",
            "Mini - Batch  117 Loss 0.018159329891204834\n",
            "Mini - Batch  118 Loss 0.03000187873840332\n",
            "Mini - Batch  119 Loss 0.039505183696746826\n",
            "Mini - Batch  120 Loss 0.02928227186203003\n",
            "Mini - Batch  121 Loss 0.02369213104248047\n",
            "Mini - Batch  122 Loss 0.019723236560821533\n",
            "Mini - Batch  123 Loss 0.015732765197753906\n",
            "Mini - Batch  124 Loss 0.02455359697341919\n",
            "Mini - Batch  125 Loss 0.026816606521606445\n",
            "Mini - Batch  126 Loss 0.027869701385498047\n",
            "Mini - Batch  127 Loss 0.01774078607559204\n",
            "Mini - Batch  128 Loss 0.02294069528579712\n",
            "Mini - Batch  129 Loss 0.015994608402252197\n",
            "Mini - Batch  130 Loss 0.03062647581100464\n",
            "Mini - Batch  131 Loss 0.020721077919006348\n",
            "Mini - Batch  132 Loss 0.025465786457061768\n",
            "Mini - Batch  133 Loss 0.0334354043006897\n",
            "Mini - Batch  134 Loss 0.0662958025932312\n",
            "Mini - Batch  135 Loss 0.01733940839767456\n",
            "Mini - Batch  136 Loss 0.02296537160873413\n",
            "Mini - Batch  137 Loss 0.031340181827545166\n",
            "Mini - Batch  138 Loss 0.015726923942565918\n",
            "Mini - Batch  139 Loss 0.029427945613861084\n",
            "Mini - Batch  140 Loss 0.15926802158355713\n",
            "Mini - Batch  141 Loss 0.04847824573516846\n",
            "Mini - Batch  142 Loss 0.03278791904449463\n",
            "Mini - Batch  143 Loss 0.02871549129486084\n",
            "Mini - Batch  144 Loss 0.04442095756530762\n",
            "Mini - Batch  145 Loss 0.040560901165008545\n",
            "Mini - Batch  146 Loss 0.024254918098449707\n",
            "Mini - Batch  147 Loss 0.026580393314361572\n",
            "Mini - Batch  148 Loss 0.02876359224319458\n",
            "Mini - Batch  149 Loss 0.025159120559692383\n",
            "Mini - Batch  150 Loss 0.03371399641036987\n",
            "Mini - Batch  151 Loss 0.017659544944763184\n",
            "Mini - Batch  152 Loss 0.03248012065887451\n",
            "Mini - Batch  153 Loss 0.022423982620239258\n",
            "Mini - Batch  154 Loss 0.02640819549560547\n",
            "Mini - Batch  155 Loss 0.02592390775680542\n",
            "Mini - Batch  156 Loss 0.02867668867111206\n",
            "Mini - Batch  157 Loss 0.027037203311920166\n",
            "Mini - Batch  158 Loss 0.04916512966156006\n",
            "Mini - Batch  159 Loss 0.02403247356414795\n",
            "Mini - Batch  160 Loss 0.034601807594299316\n",
            "Mini - Batch  161 Loss 0.055251121520996094\n",
            "Mini - Batch  162 Loss 0.02549576759338379\n",
            "Mini - Batch  163 Loss 0.016525566577911377\n",
            "Mini - Batch  164 Loss 0.021457016468048096\n",
            "Mini - Batch  165 Loss 0.025060832500457764\n",
            "Mini - Batch  166 Loss 0.08914071321487427\n",
            "\t Epoch Loss = [8] loss: 5.009\n",
            "Mini - Batch  0 Loss 0.017071545124053955\n",
            "Mini - Batch  1 Loss 0.06422054767608643\n",
            "Mini - Batch  2 Loss 0.021579384803771973\n",
            "Mini - Batch  3 Loss 0.024469375610351562\n",
            "Mini - Batch  4 Loss 0.019522130489349365\n",
            "Mini - Batch  5 Loss 0.03097403049468994\n",
            "Mini - Batch  6 Loss 0.02409350872039795\n",
            "Mini - Batch  7 Loss 0.03399491310119629\n",
            "Mini - Batch  8 Loss 0.018750131130218506\n",
            "Mini - Batch  9 Loss 0.030238747596740723\n",
            "Mini - Batch  10 Loss 0.0350765585899353\n",
            "Mini - Batch  11 Loss 0.03187203407287598\n",
            "Mini - Batch  12 Loss 0.016121864318847656\n",
            "Mini - Batch  13 Loss 0.013108313083648682\n",
            "Mini - Batch  14 Loss 0.026585400104522705\n",
            "Mini - Batch  15 Loss 0.07069379091262817\n",
            "Mini - Batch  16 Loss 0.030864417552947998\n",
            "Mini - Batch  17 Loss 0.026161670684814453\n",
            "Mini - Batch  18 Loss 0.029883205890655518\n",
            "Mini - Batch  19 Loss 0.02553379535675049\n",
            "Mini - Batch  20 Loss 0.02484571933746338\n",
            "Mini - Batch  21 Loss 0.017451047897338867\n",
            "Mini - Batch  22 Loss 0.017868757247924805\n",
            "Mini - Batch  23 Loss 0.025646686553955078\n",
            "Mini - Batch  24 Loss 0.024565279483795166\n",
            "Mini - Batch  25 Loss 0.022961914539337158\n",
            "Mini - Batch  26 Loss 0.021610021591186523\n",
            "Mini - Batch  27 Loss 0.025011777877807617\n",
            "Mini - Batch  28 Loss 0.02555292844772339\n",
            "Mini - Batch  29 Loss 0.01769077777862549\n",
            "Mini - Batch  30 Loss 0.023519575595855713\n",
            "Mini - Batch  31 Loss 0.025049865245819092\n",
            "Mini - Batch  32 Loss 0.026086628437042236\n",
            "Mini - Batch  33 Loss 0.02432018518447876\n",
            "Mini - Batch  34 Loss 0.05773568153381348\n",
            "Mini - Batch  35 Loss 0.025525927543640137\n",
            "Mini - Batch  36 Loss 0.049977004528045654\n",
            "Mini - Batch  37 Loss 0.03205764293670654\n",
            "Mini - Batch  38 Loss 0.04054349660873413\n",
            "Mini - Batch  39 Loss 0.02685469388961792\n",
            "Mini - Batch  40 Loss 0.02037215232849121\n",
            "Mini - Batch  41 Loss 0.02081841230392456\n",
            "Mini - Batch  42 Loss 0.030440211296081543\n",
            "Mini - Batch  43 Loss 0.02294456958770752\n",
            "Mini - Batch  44 Loss 0.025995314121246338\n",
            "Mini - Batch  45 Loss 0.0390591025352478\n",
            "Mini - Batch  46 Loss 0.02347421646118164\n",
            "Mini - Batch  47 Loss 0.03215062618255615\n",
            "Mini - Batch  48 Loss 0.04415494203567505\n",
            "Mini - Batch  49 Loss 0.02348649501800537\n",
            "Mini - Batch  50 Loss 0.028929948806762695\n",
            "Mini - Batch  51 Loss 0.020590782165527344\n",
            "Mini - Batch  52 Loss 0.0420917272567749\n",
            "Mini - Batch  53 Loss 0.053082048892974854\n",
            "Mini - Batch  54 Loss 0.0367770791053772\n",
            "Mini - Batch  55 Loss 0.03286087512969971\n",
            "Mini - Batch  56 Loss 0.02020663022994995\n",
            "Mini - Batch  57 Loss 0.1262626051902771\n",
            "Mini - Batch  58 Loss 0.03684478998184204\n",
            "Mini - Batch  59 Loss 0.042498886585235596\n",
            "Mini - Batch  60 Loss 0.0215265154838562\n",
            "Mini - Batch  61 Loss 0.020323455333709717\n",
            "Mini - Batch  62 Loss 0.01894146203994751\n",
            "Mini - Batch  63 Loss 0.05580019950866699\n",
            "Mini - Batch  64 Loss 0.0327451229095459\n",
            "Mini - Batch  65 Loss 0.025534749031066895\n",
            "Mini - Batch  66 Loss 0.021960914134979248\n",
            "Mini - Batch  67 Loss 0.017348885536193848\n",
            "Mini - Batch  68 Loss 0.05577659606933594\n",
            "Mini - Batch  69 Loss 0.04626452922821045\n",
            "Mini - Batch  70 Loss 0.021618545055389404\n",
            "Mini - Batch  71 Loss 0.02336430549621582\n",
            "Mini - Batch  72 Loss 0.02563875913619995\n",
            "Mini - Batch  73 Loss 0.018218994140625\n",
            "Mini - Batch  74 Loss 0.02955019474029541\n",
            "Mini - Batch  75 Loss 0.036404311656951904\n",
            "Mini - Batch  76 Loss 0.020429790019989014\n",
            "Mini - Batch  77 Loss 0.046250224113464355\n",
            "Mini - Batch  78 Loss 0.018671035766601562\n",
            "Mini - Batch  79 Loss 0.022382915019989014\n",
            "Mini - Batch  80 Loss 0.049336254596710205\n",
            "Mini - Batch  81 Loss 0.023079991340637207\n",
            "Mini - Batch  82 Loss 0.015142738819122314\n",
            "Mini - Batch  83 Loss 0.01885122060775757\n",
            "Mini - Batch  84 Loss 0.03134047985076904\n",
            "Mini - Batch  85 Loss 0.03744298219680786\n",
            "Mini - Batch  86 Loss 0.01719069480895996\n",
            "Mini - Batch  87 Loss 0.021625936031341553\n",
            "Mini - Batch  88 Loss 0.017455637454986572\n",
            "Mini - Batch  89 Loss 0.022155165672302246\n",
            "Mini - Batch  90 Loss 0.06913226842880249\n",
            "Mini - Batch  91 Loss 0.02580249309539795\n",
            "Mini - Batch  92 Loss 0.14678055047988892\n",
            "Mini - Batch  93 Loss 0.013962984085083008\n",
            "Mini - Batch  94 Loss 0.04494732618331909\n",
            "Mini - Batch  95 Loss 0.020496666431427002\n",
            "Mini - Batch  96 Loss 0.016649603843688965\n",
            "Mini - Batch  97 Loss 0.021033823490142822\n",
            "Mini - Batch  98 Loss 0.023644208908081055\n",
            "Mini - Batch  99 Loss 0.01825857162475586\n",
            "Mini - Batch  100 Loss 0.020988643169403076\n",
            "Mini - Batch  101 Loss 0.02135336399078369\n",
            "Mini - Batch  102 Loss 0.07858788967132568\n",
            "Mini - Batch  103 Loss 0.040877461433410645\n",
            "Mini - Batch  104 Loss 0.05961674451828003\n",
            "Mini - Batch  105 Loss 0.030257821083068848\n",
            "Mini - Batch  106 Loss 0.017993509769439697\n",
            "Mini - Batch  107 Loss 0.050079941749572754\n",
            "Mini - Batch  108 Loss 0.049237191677093506\n",
            "Mini - Batch  109 Loss 0.05502408742904663\n",
            "Mini - Batch  110 Loss 0.019467830657958984\n",
            "Mini - Batch  111 Loss 0.025804996490478516\n",
            "Mini - Batch  112 Loss 0.03393751382827759\n",
            "Mini - Batch  113 Loss 0.043965816497802734\n",
            "Mini - Batch  114 Loss 0.01656806468963623\n",
            "Mini - Batch  115 Loss 0.027477681636810303\n",
            "Mini - Batch  116 Loss 0.019909799098968506\n",
            "Mini - Batch  117 Loss 0.017911851406097412\n",
            "Mini - Batch  118 Loss 0.021106958389282227\n",
            "Mini - Batch  119 Loss 0.03330588340759277\n",
            "Mini - Batch  120 Loss 0.1224602460861206\n",
            "Mini - Batch  121 Loss 0.024794042110443115\n",
            "Mini - Batch  122 Loss 0.0325697660446167\n",
            "Mini - Batch  123 Loss 0.05553930997848511\n",
            "Mini - Batch  124 Loss 0.03635638952255249\n",
            "Mini - Batch  125 Loss 0.030583500862121582\n",
            "Mini - Batch  126 Loss 0.022408664226531982\n",
            "Mini - Batch  127 Loss 0.02732938528060913\n",
            "Mini - Batch  128 Loss 0.05704092979431152\n",
            "Mini - Batch  129 Loss 0.042519986629486084\n",
            "Mini - Batch  130 Loss 0.04552716016769409\n",
            "Mini - Batch  131 Loss 0.1458808183670044\n",
            "Mini - Batch  132 Loss 0.021239042282104492\n",
            "Mini - Batch  133 Loss 0.03583401441574097\n",
            "Mini - Batch  134 Loss 0.018457472324371338\n",
            "Mini - Batch  135 Loss 0.024516820907592773\n",
            "Mini - Batch  136 Loss 0.04238539934158325\n",
            "Mini - Batch  137 Loss 0.02459537982940674\n",
            "Mini - Batch  138 Loss 0.025988101959228516\n",
            "Mini - Batch  139 Loss 0.02943819761276245\n",
            "Mini - Batch  140 Loss 0.033624887466430664\n",
            "Mini - Batch  141 Loss 0.0241052508354187\n",
            "Mini - Batch  142 Loss 0.04205566644668579\n",
            "Mini - Batch  143 Loss 0.03250032663345337\n",
            "Mini - Batch  144 Loss 0.049277305603027344\n",
            "Mini - Batch  145 Loss 0.023142457008361816\n",
            "Mini - Batch  146 Loss 0.021840035915374756\n",
            "Mini - Batch  147 Loss 0.02929919958114624\n",
            "Mini - Batch  148 Loss 0.038264691829681396\n",
            "Mini - Batch  149 Loss 0.04058575630187988\n",
            "Mini - Batch  150 Loss 0.021024346351623535\n",
            "Mini - Batch  151 Loss 0.01967167854309082\n",
            "Mini - Batch  152 Loss 0.04057180881500244\n",
            "Mini - Batch  153 Loss 0.13647723197937012\n",
            "Mini - Batch  154 Loss 0.020051002502441406\n",
            "Mini - Batch  155 Loss 0.031414806842803955\n",
            "Mini - Batch  156 Loss 0.03806769847869873\n",
            "Mini - Batch  157 Loss 0.024704694747924805\n",
            "Mini - Batch  158 Loss 0.04628342390060425\n",
            "Mini - Batch  159 Loss 0.03890872001647949\n",
            "Mini - Batch  160 Loss 0.034991562366485596\n",
            "Mini - Batch  161 Loss 0.025388896465301514\n",
            "Mini - Batch  162 Loss 0.03436017036437988\n",
            "Mini - Batch  163 Loss 0.049472033977508545\n",
            "Mini - Batch  164 Loss 0.02972996234893799\n",
            "Mini - Batch  165 Loss 0.03170156478881836\n",
            "Mini - Batch  166 Loss 0.07207721471786499\n",
            "\t Epoch Loss = [9] loss: 5.680\n",
            "Mini - Batch  0 Loss 0.023887336254119873\n",
            "Mini - Batch  1 Loss 0.019760608673095703\n",
            "Mini - Batch  2 Loss 0.024860918521881104\n",
            "Mini - Batch  3 Loss 0.1336594820022583\n",
            "Mini - Batch  4 Loss 0.024680256843566895\n",
            "Mini - Batch  5 Loss 0.025715231895446777\n",
            "Mini - Batch  6 Loss 0.021103084087371826\n",
            "Mini - Batch  7 Loss 0.022537708282470703\n",
            "Mini - Batch  8 Loss 0.0633624792098999\n",
            "Mini - Batch  9 Loss 0.026515603065490723\n",
            "Mini - Batch  10 Loss 0.028988778591156006\n",
            "Mini - Batch  11 Loss 0.03059542179107666\n",
            "Mini - Batch  12 Loss 0.027606844902038574\n",
            "Mini - Batch  13 Loss 0.026585102081298828\n",
            "Mini - Batch  14 Loss 0.019725680351257324\n",
            "Mini - Batch  15 Loss 0.020105183124542236\n",
            "Mini - Batch  16 Loss 0.056103527545928955\n",
            "Mini - Batch  17 Loss 0.031158089637756348\n",
            "Mini - Batch  18 Loss 0.01937544345855713\n",
            "Mini - Batch  19 Loss 0.02335059642791748\n",
            "Mini - Batch  20 Loss 0.022718369960784912\n",
            "Mini - Batch  21 Loss 0.04014497995376587\n",
            "Mini - Batch  22 Loss 0.060410916805267334\n",
            "Mini - Batch  23 Loss 0.027789771556854248\n",
            "Mini - Batch  24 Loss 0.030474424362182617\n",
            "Mini - Batch  25 Loss 0.01908552646636963\n",
            "Mini - Batch  26 Loss 0.05973231792449951\n",
            "Mini - Batch  27 Loss 0.025210440158843994\n",
            "Mini - Batch  28 Loss 0.027174711227416992\n",
            "Mini - Batch  29 Loss 0.03983175754547119\n",
            "Mini - Batch  30 Loss 0.03598600625991821\n",
            "Mini - Batch  31 Loss 0.02017807960510254\n",
            "Mini - Batch  32 Loss 0.025887727737426758\n",
            "Mini - Batch  33 Loss 0.023935139179229736\n",
            "Mini - Batch  34 Loss 0.02838224172592163\n",
            "Mini - Batch  35 Loss 0.05880087614059448\n",
            "Mini - Batch  36 Loss 0.0245552659034729\n",
            "Mini - Batch  37 Loss 0.03123021125793457\n",
            "Mini - Batch  38 Loss 0.023297369480133057\n",
            "Mini - Batch  39 Loss 0.030948340892791748\n",
            "Mini - Batch  40 Loss 0.024432122707366943\n",
            "Mini - Batch  41 Loss 0.018501758575439453\n",
            "Mini - Batch  42 Loss 0.02471846342086792\n",
            "Mini - Batch  43 Loss 0.029507040977478027\n",
            "Mini - Batch  44 Loss 0.026278376579284668\n",
            "Mini - Batch  45 Loss 0.029551148414611816\n",
            "Mini - Batch  46 Loss 0.02543330192565918\n",
            "Mini - Batch  47 Loss 0.030625760555267334\n",
            "Mini - Batch  48 Loss 0.055209577083587646\n",
            "Mini - Batch  49 Loss 0.02888333797454834\n",
            "Mini - Batch  50 Loss 0.02829134464263916\n",
            "Mini - Batch  51 Loss 0.02206242084503174\n",
            "Mini - Batch  52 Loss 0.03420823812484741\n",
            "Mini - Batch  53 Loss 0.01971989870071411\n",
            "Mini - Batch  54 Loss 0.12677037715911865\n",
            "Mini - Batch  55 Loss 0.04203653335571289\n",
            "Mini - Batch  56 Loss 0.03154158592224121\n",
            "Mini - Batch  57 Loss 0.03918313980102539\n",
            "Mini - Batch  58 Loss 0.02698683738708496\n",
            "Mini - Batch  59 Loss 0.019813179969787598\n",
            "Mini - Batch  60 Loss 0.021606862545013428\n",
            "Mini - Batch  61 Loss 0.030687689781188965\n",
            "Mini - Batch  62 Loss 0.02502232789993286\n",
            "Mini - Batch  63 Loss 0.05348259210586548\n",
            "Mini - Batch  64 Loss 0.01298755407333374\n",
            "Mini - Batch  65 Loss 0.0193101167678833\n",
            "Mini - Batch  66 Loss 0.03265941143035889\n",
            "Mini - Batch  67 Loss 0.03276175260543823\n",
            "Mini - Batch  68 Loss 0.03230816125869751\n",
            "Mini - Batch  69 Loss 0.025858819484710693\n",
            "Mini - Batch  70 Loss 0.03523063659667969\n",
            "Mini - Batch  71 Loss 0.02403402328491211\n",
            "Mini - Batch  72 Loss 0.09805417060852051\n",
            "Mini - Batch  73 Loss 0.015060126781463623\n",
            "Mini - Batch  74 Loss 0.022066175937652588\n",
            "Mini - Batch  75 Loss 0.021871566772460938\n",
            "Mini - Batch  76 Loss 0.024793624877929688\n",
            "Mini - Batch  77 Loss 0.023332417011260986\n",
            "Mini - Batch  78 Loss 0.03294771909713745\n",
            "Mini - Batch  79 Loss 0.019031226634979248\n",
            "Mini - Batch  80 Loss 0.032859086990356445\n",
            "Mini - Batch  81 Loss 0.021362721920013428\n",
            "Mini - Batch  82 Loss 0.025362491607666016\n",
            "Mini - Batch  83 Loss 0.024324476718902588\n",
            "Mini - Batch  84 Loss 0.01979994773864746\n",
            "Mini - Batch  85 Loss 0.022845804691314697\n",
            "Mini - Batch  86 Loss 0.03037118911743164\n",
            "Mini - Batch  87 Loss 0.018418729305267334\n",
            "Mini - Batch  88 Loss 0.032496869564056396\n",
            "Mini - Batch  89 Loss 0.021609485149383545\n",
            "Mini - Batch  90 Loss 0.05608510971069336\n",
            "Mini - Batch  91 Loss 0.0250246524810791\n",
            "Mini - Batch  92 Loss 0.02140367031097412\n",
            "Mini - Batch  93 Loss 0.020771384239196777\n",
            "Mini - Batch  94 Loss 0.025021791458129883\n",
            "Mini - Batch  95 Loss 0.02151620388031006\n",
            "Mini - Batch  96 Loss 0.02515435218811035\n",
            "Mini - Batch  97 Loss 0.029429495334625244\n",
            "Mini - Batch  98 Loss 0.046183645725250244\n",
            "Mini - Batch  99 Loss 0.026782333850860596\n",
            "Mini - Batch  100 Loss 0.01976388692855835\n",
            "Mini - Batch  101 Loss 0.03186368942260742\n",
            "Mini - Batch  102 Loss 0.04414868354797363\n",
            "Mini - Batch  103 Loss 0.046966493129730225\n",
            "Mini - Batch  104 Loss 0.01723116636276245\n",
            "Mini - Batch  105 Loss 0.022178828716278076\n",
            "Mini - Batch  106 Loss 0.03273296356201172\n",
            "Mini - Batch  107 Loss 0.02304285764694214\n",
            "Mini - Batch  108 Loss 0.01466989517211914\n",
            "Mini - Batch  109 Loss 0.022791922092437744\n",
            "Mini - Batch  110 Loss 0.0375174880027771\n",
            "Mini - Batch  111 Loss 0.024133265018463135\n",
            "Mini - Batch  112 Loss 0.05730879306793213\n",
            "Mini - Batch  113 Loss 0.019509077072143555\n",
            "Mini - Batch  114 Loss 0.01731795072555542\n",
            "Mini - Batch  115 Loss 0.019996285438537598\n",
            "Mini - Batch  116 Loss 0.017247915267944336\n",
            "Mini - Batch  117 Loss 0.022290050983428955\n",
            "Mini - Batch  118 Loss 0.020707130432128906\n",
            "Mini - Batch  119 Loss 0.025959551334381104\n",
            "Mini - Batch  120 Loss 0.014751136302947998\n",
            "Mini - Batch  121 Loss 0.019418001174926758\n",
            "Mini - Batch  122 Loss 0.04484623670578003\n",
            "Mini - Batch  123 Loss 0.023168087005615234\n",
            "Mini - Batch  124 Loss 0.023464202880859375\n",
            "Mini - Batch  125 Loss 0.05434679985046387\n",
            "Mini - Batch  126 Loss 0.03503882884979248\n",
            "Mini - Batch  127 Loss 0.019165098667144775\n",
            "Mini - Batch  128 Loss 0.02686750888824463\n",
            "Mini - Batch  129 Loss 0.025533854961395264\n",
            "Mini - Batch  130 Loss 0.02286994457244873\n",
            "Mini - Batch  131 Loss 0.029345214366912842\n",
            "Mini - Batch  132 Loss 0.02882099151611328\n",
            "Mini - Batch  133 Loss 0.028022170066833496\n",
            "Mini - Batch  134 Loss 0.03572958707809448\n",
            "Mini - Batch  135 Loss 0.04754263162612915\n",
            "Mini - Batch  136 Loss 0.01367652416229248\n",
            "Mini - Batch  137 Loss 0.019541501998901367\n",
            "Mini - Batch  138 Loss 0.023290514945983887\n",
            "Mini - Batch  139 Loss 0.02803981304168701\n",
            "Mini - Batch  140 Loss 0.03860074281692505\n",
            "Mini - Batch  141 Loss 0.02580416202545166\n",
            "Mini - Batch  142 Loss 0.03071528673171997\n",
            "Mini - Batch  143 Loss 0.028916358947753906\n",
            "Mini - Batch  144 Loss 0.037649691104888916\n",
            "Mini - Batch  145 Loss 0.05453801155090332\n",
            "Mini - Batch  146 Loss 0.03891396522521973\n",
            "Mini - Batch  147 Loss 0.017481207847595215\n",
            "Mini - Batch  148 Loss 0.039905548095703125\n",
            "Mini - Batch  149 Loss 0.018613457679748535\n",
            "Mini - Batch  150 Loss 0.03068864345550537\n",
            "Mini - Batch  151 Loss 0.025721490383148193\n",
            "Mini - Batch  152 Loss 0.016384780406951904\n",
            "Mini - Batch  153 Loss 0.041831254959106445\n",
            "Mini - Batch  154 Loss 0.01669490337371826\n",
            "Mini - Batch  155 Loss 0.020486533641815186\n",
            "Mini - Batch  156 Loss 0.027814090251922607\n",
            "Mini - Batch  157 Loss 0.024930596351623535\n",
            "Mini - Batch  158 Loss 0.0200464129447937\n",
            "Mini - Batch  159 Loss 0.04199439287185669\n",
            "Mini - Batch  160 Loss 0.034299492835998535\n",
            "Mini - Batch  161 Loss 0.023723304271697998\n",
            "Mini - Batch  162 Loss 0.15016478300094604\n",
            "Mini - Batch  163 Loss 0.019123494625091553\n",
            "Mini - Batch  164 Loss 0.017906248569488525\n",
            "Mini - Batch  165 Loss 0.01832437515258789\n",
            "Mini - Batch  166 Loss 0.040202796459198\n",
            "\t Epoch Loss = [10] loss: 5.161\n",
            "Mini - Batch  0 Loss 0.03971296548843384\n",
            "Mini - Batch  1 Loss 0.023700237274169922\n",
            "Mini - Batch  2 Loss 0.029153645038604736\n",
            "Mini - Batch  3 Loss 0.023119807243347168\n",
            "Mini - Batch  4 Loss 0.020535528659820557\n",
            "Mini - Batch  5 Loss 0.03393423557281494\n",
            "Mini - Batch  6 Loss 0.025248050689697266\n",
            "Mini - Batch  7 Loss 0.02451777458190918\n",
            "Mini - Batch  8 Loss 0.018879473209381104\n",
            "Mini - Batch  9 Loss 0.022911489009857178\n",
            "Mini - Batch  10 Loss 0.020265281200408936\n",
            "Mini - Batch  11 Loss 0.036873459815979004\n",
            "Mini - Batch  12 Loss 0.02204596996307373\n",
            "Mini - Batch  13 Loss 0.023306846618652344\n",
            "Mini - Batch  14 Loss 0.031193554401397705\n",
            "Mini - Batch  15 Loss 0.01542973518371582\n",
            "Mini - Batch  16 Loss 0.025624752044677734\n",
            "Mini - Batch  17 Loss 0.03532874584197998\n",
            "Mini - Batch  18 Loss 0.02806776762008667\n",
            "Mini - Batch  19 Loss 0.027568817138671875\n",
            "Mini - Batch  20 Loss 0.03658205270767212\n",
            "Mini - Batch  21 Loss 0.021615207195281982\n",
            "Mini - Batch  22 Loss 0.01543647050857544\n",
            "Mini - Batch  23 Loss 0.02152538299560547\n",
            "Mini - Batch  24 Loss 0.03131663799285889\n",
            "Mini - Batch  25 Loss 0.04208296537399292\n",
            "Mini - Batch  26 Loss 0.02285987138748169\n",
            "Mini - Batch  27 Loss 0.02073568105697632\n",
            "Mini - Batch  28 Loss 0.02023029327392578\n",
            "Mini - Batch  29 Loss 0.03122997283935547\n",
            "Mini - Batch  30 Loss 0.025211989879608154\n",
            "Mini - Batch  31 Loss 0.02184218168258667\n",
            "Mini - Batch  32 Loss 0.039003729820251465\n",
            "Mini - Batch  33 Loss 0.03098076581954956\n",
            "Mini - Batch  34 Loss 0.04224956035614014\n",
            "Mini - Batch  35 Loss 0.01839214563369751\n",
            "Mini - Batch  36 Loss 0.027785539627075195\n",
            "Mini - Batch  37 Loss 0.0169488787651062\n",
            "Mini - Batch  38 Loss 0.025999367237091064\n",
            "Mini - Batch  39 Loss 0.022638142108917236\n",
            "Mini - Batch  40 Loss 0.021980106830596924\n",
            "Mini - Batch  41 Loss 0.15266650915145874\n",
            "Mini - Batch  42 Loss 0.02945232391357422\n",
            "Mini - Batch  43 Loss 0.02388542890548706\n",
            "Mini - Batch  44 Loss 0.018734633922576904\n",
            "Mini - Batch  45 Loss 0.04535627365112305\n",
            "Mini - Batch  46 Loss 0.01908785104751587\n",
            "Mini - Batch  47 Loss 0.01824164390563965\n",
            "Mini - Batch  48 Loss 0.02742898464202881\n",
            "Mini - Batch  49 Loss 0.025636374950408936\n",
            "Mini - Batch  50 Loss 0.025013327598571777\n",
            "Mini - Batch  51 Loss 0.027864158153533936\n",
            "Mini - Batch  52 Loss 0.02543330192565918\n",
            "Mini - Batch  53 Loss 0.02480614185333252\n",
            "Mini - Batch  54 Loss 0.02552086114883423\n",
            "Mini - Batch  55 Loss 0.03303486108779907\n",
            "Mini - Batch  56 Loss 0.015574336051940918\n",
            "Mini - Batch  57 Loss 0.02677851915359497\n",
            "Mini - Batch  58 Loss 0.015874147415161133\n",
            "Mini - Batch  59 Loss 0.027920186519622803\n",
            "Mini - Batch  60 Loss 0.054432570934295654\n",
            "Mini - Batch  61 Loss 0.026627182960510254\n",
            "Mini - Batch  62 Loss 0.025640547275543213\n",
            "Mini - Batch  63 Loss 0.03682136535644531\n",
            "Mini - Batch  64 Loss 0.04486095905303955\n",
            "Mini - Batch  65 Loss 0.031579017639160156\n",
            "Mini - Batch  66 Loss 0.055540382862091064\n",
            "Mini - Batch  67 Loss 0.0190773606300354\n",
            "Mini - Batch  68 Loss 0.02223604917526245\n",
            "Mini - Batch  69 Loss 0.027630925178527832\n",
            "Mini - Batch  70 Loss 0.03642749786376953\n",
            "Mini - Batch  71 Loss 0.04139244556427002\n",
            "Mini - Batch  72 Loss 0.12867248058319092\n",
            "Mini - Batch  73 Loss 0.017498373985290527\n",
            "Mini - Batch  74 Loss 0.02925974130630493\n",
            "Mini - Batch  75 Loss 0.030330657958984375\n",
            "Mini - Batch  76 Loss 0.03065389394760132\n",
            "Mini - Batch  77 Loss 0.017601430416107178\n",
            "Mini - Batch  78 Loss 0.022239208221435547\n",
            "Mini - Batch  79 Loss 0.02684849500656128\n",
            "Mini - Batch  80 Loss 0.03784894943237305\n",
            "Mini - Batch  81 Loss 0.027862131595611572\n",
            "Mini - Batch  82 Loss 0.02315741777420044\n",
            "Mini - Batch  83 Loss 0.01982712745666504\n",
            "Mini - Batch  84 Loss 0.021092355251312256\n",
            "Mini - Batch  85 Loss 0.034551799297332764\n",
            "Mini - Batch  86 Loss 0.0266038179397583\n",
            "Mini - Batch  87 Loss 0.02036881446838379\n",
            "Mini - Batch  88 Loss 0.017272651195526123\n",
            "Mini - Batch  89 Loss 0.02076268196105957\n",
            "Mini - Batch  90 Loss 0.028261542320251465\n",
            "Mini - Batch  91 Loss 0.05362445116043091\n",
            "Mini - Batch  92 Loss 0.05565142631530762\n",
            "Mini - Batch  93 Loss 0.020970463752746582\n",
            "Mini - Batch  94 Loss 0.0905156135559082\n",
            "Mini - Batch  95 Loss 0.0326424241065979\n",
            "Mini - Batch  96 Loss 0.03371709585189819\n",
            "Mini - Batch  97 Loss 0.03342932462692261\n",
            "Mini - Batch  98 Loss 0.05631136894226074\n",
            "Mini - Batch  99 Loss 0.05173897743225098\n",
            "Mini - Batch  100 Loss 0.03703111410140991\n",
            "Mini - Batch  101 Loss 0.0426863431930542\n",
            "Mini - Batch  102 Loss 0.027659475803375244\n",
            "Mini - Batch  103 Loss 0.03069603443145752\n",
            "Mini - Batch  104 Loss 0.040271878242492676\n",
            "Mini - Batch  105 Loss 0.02738875150680542\n",
            "Mini - Batch  106 Loss 0.020443618297576904\n",
            "Mini - Batch  107 Loss 0.01901376247406006\n",
            "Mini - Batch  108 Loss 0.03251737356185913\n",
            "Mini - Batch  109 Loss 0.03140002489089966\n",
            "Mini - Batch  110 Loss 0.01704782247543335\n",
            "Mini - Batch  111 Loss 0.04057997465133667\n",
            "Mini - Batch  112 Loss 0.023043811321258545\n",
            "Mini - Batch  113 Loss 0.023603498935699463\n",
            "Mini - Batch  114 Loss 0.020572006702423096\n",
            "Mini - Batch  115 Loss 0.014560580253601074\n",
            "Mini - Batch  116 Loss 0.02319544553756714\n",
            "Mini - Batch  117 Loss 0.0275113582611084\n",
            "Mini - Batch  118 Loss 0.02359718084335327\n",
            "Mini - Batch  119 Loss 0.017512202262878418\n",
            "Mini - Batch  120 Loss 0.0513727068901062\n",
            "Mini - Batch  121 Loss 0.04716378450393677\n",
            "Mini - Batch  122 Loss 0.02718639373779297\n",
            "Mini - Batch  123 Loss 0.019808411598205566\n",
            "Mini - Batch  124 Loss 0.03654134273529053\n",
            "Mini - Batch  125 Loss 0.10215616226196289\n",
            "Mini - Batch  126 Loss 0.03435307741165161\n",
            "Mini - Batch  127 Loss 0.018440663814544678\n",
            "Mini - Batch  128 Loss 0.015609025955200195\n",
            "Mini - Batch  129 Loss 0.037654101848602295\n",
            "Mini - Batch  130 Loss 0.040939390659332275\n",
            "Mini - Batch  131 Loss 0.025656521320343018\n",
            "Mini - Batch  132 Loss 0.05344611406326294\n",
            "Mini - Batch  133 Loss 0.014230430126190186\n",
            "Mini - Batch  134 Loss 0.027083396911621094\n",
            "Mini - Batch  135 Loss 0.02455061674118042\n",
            "Mini - Batch  136 Loss 0.022783100605010986\n",
            "Mini - Batch  137 Loss 0.025061368942260742\n",
            "Mini - Batch  138 Loss 0.03292721509933472\n",
            "Mini - Batch  139 Loss 0.026365041732788086\n",
            "Mini - Batch  140 Loss 0.014275193214416504\n",
            "Mini - Batch  141 Loss 0.046395719051361084\n",
            "Mini - Batch  142 Loss 0.02607792615890503\n",
            "Mini - Batch  143 Loss 0.015755951404571533\n",
            "Mini - Batch  144 Loss 0.06162369251251221\n",
            "Mini - Batch  145 Loss 0.021205008029937744\n",
            "Mini - Batch  146 Loss 0.019874930381774902\n",
            "Mini - Batch  147 Loss 0.01741933822631836\n",
            "Mini - Batch  148 Loss 0.02903008460998535\n",
            "Mini - Batch  149 Loss 0.02227151393890381\n",
            "Mini - Batch  150 Loss 0.032154083251953125\n",
            "Mini - Batch  151 Loss 0.03453892469406128\n",
            "Mini - Batch  152 Loss 0.027958452701568604\n",
            "Mini - Batch  153 Loss 0.02747243642807007\n",
            "Mini - Batch  154 Loss 0.02381378412246704\n",
            "Mini - Batch  155 Loss 0.04161101579666138\n",
            "Mini - Batch  156 Loss 0.050857603549957275\n",
            "Mini - Batch  157 Loss 0.0328749418258667\n",
            "Mini - Batch  158 Loss 0.019592106342315674\n",
            "Mini - Batch  159 Loss 0.02845144271850586\n",
            "Mini - Batch  160 Loss 0.04192471504211426\n",
            "Mini - Batch  161 Loss 0.04982876777648926\n",
            "Mini - Batch  162 Loss 0.018778622150421143\n",
            "Mini - Batch  163 Loss 0.01996469497680664\n",
            "Mini - Batch  164 Loss 0.13335543870925903\n",
            "Mini - Batch  165 Loss 0.034079134464263916\n",
            "Mini - Batch  166 Loss 0.019222021102905273\n",
            "\t Epoch Loss = [11] loss: 5.258\n",
            "Mini - Batch  0 Loss 0.022892415523529053\n",
            "Mini - Batch  1 Loss 0.01990264654159546\n",
            "Mini - Batch  2 Loss 0.02034527063369751\n",
            "Mini - Batch  3 Loss 0.016312599182128906\n",
            "Mini - Batch  4 Loss 0.018512606620788574\n",
            "Mini - Batch  5 Loss 0.038596391677856445\n",
            "Mini - Batch  6 Loss 0.034931480884552\n",
            "Mini - Batch  7 Loss 0.02919185161590576\n",
            "Mini - Batch  8 Loss 0.023048222064971924\n",
            "Mini - Batch  9 Loss 0.033703744411468506\n",
            "Mini - Batch  10 Loss 0.028784215450286865\n",
            "Mini - Batch  11 Loss 0.02431666851043701\n",
            "Mini - Batch  12 Loss 0.03924751281738281\n",
            "Mini - Batch  13 Loss 0.024863779544830322\n",
            "Mini - Batch  14 Loss 0.021321654319763184\n",
            "Mini - Batch  15 Loss 0.03594917058944702\n",
            "Mini - Batch  16 Loss 0.025779306888580322\n",
            "Mini - Batch  17 Loss 0.01838827133178711\n",
            "Mini - Batch  18 Loss 0.01982492208480835\n",
            "Mini - Batch  19 Loss 0.018792808055877686\n",
            "Mini - Batch  20 Loss 0.016769230365753174\n",
            "Mini - Batch  21 Loss 0.023756325244903564\n",
            "Mini - Batch  22 Loss 0.025378048419952393\n",
            "Mini - Batch  23 Loss 0.02807176113128662\n",
            "Mini - Batch  24 Loss 0.017633497714996338\n",
            "Mini - Batch  25 Loss 0.03215515613555908\n",
            "Mini - Batch  26 Loss 0.03483372926712036\n",
            "Mini - Batch  27 Loss 0.023839354515075684\n",
            "Mini - Batch  28 Loss 0.04189223051071167\n",
            "Mini - Batch  29 Loss 0.021942973136901855\n",
            "Mini - Batch  30 Loss 0.022835195064544678\n",
            "Mini - Batch  31 Loss 0.021383583545684814\n",
            "Mini - Batch  32 Loss 0.019499540328979492\n",
            "Mini - Batch  33 Loss 0.01989912986755371\n",
            "Mini - Batch  34 Loss 0.027175724506378174\n",
            "Mini - Batch  35 Loss 0.022484958171844482\n",
            "Mini - Batch  36 Loss 0.02104794979095459\n",
            "Mini - Batch  37 Loss 0.022011995315551758\n",
            "Mini - Batch  38 Loss 0.02963203191757202\n",
            "Mini - Batch  39 Loss 0.03503549098968506\n",
            "Mini - Batch  40 Loss 0.029483914375305176\n",
            "Mini - Batch  41 Loss 0.01976686716079712\n",
            "Mini - Batch  42 Loss 0.019180893898010254\n",
            "Mini - Batch  43 Loss 0.13964956998825073\n",
            "Mini - Batch  44 Loss 0.025337636470794678\n",
            "Mini - Batch  45 Loss 0.016403913497924805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8RoszZgCya9"
      },
      "source": [
        "# evaluate model:\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(trainLoader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        images = data['image']\n",
        "        masks = data['mask']\n",
        "        scatters = data['scatter']\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(images, scatters)\n",
        "        loss = criterion(outputs, masks)\n",
        "        print(\"Mini - Batch \",i,\"Loss\", loss.item())\n",
        "\n",
        "        # Detach\n",
        "        print(images[0].shape)\n",
        "        \n",
        "        # Plot and see\n",
        "        f, axs = plt.subplots(3,3,figsize=(15,15))\n",
        "        i = 0\n",
        "        image = transforms.ToPILImage()(images[i])\n",
        "        plt.subplot(3,3,3*i+1)\n",
        "        plt.imshow(image, 'gray')\n",
        "        mask = transforms.ToPILImage()(masks[i])\n",
        "        plt.subplot(3,3,3*i+2)\n",
        "        plt.imshow(mask)\n",
        "        output = transforms.ToPILImage()(outputs[i])\n",
        "        plt.subplot(3,3,3*i+3)\n",
        "        plt.imshow(output)\n",
        "\n",
        "        i = 0\n",
        "        image = transforms.ToPILImage()(images[i])\n",
        "        plt.subplot(3,3,3*i+1)\n",
        "        plt.imshow(image, 'gray')\n",
        "        mask = transforms.ToPILImage()(masks[i])\n",
        "        plt.subplot(3,3,3*i+2)\n",
        "        plt.imshow(mask)\n",
        "        output = transforms.ToPILImage()(outputs[i])\n",
        "        plt.subplot(3,3,3*i+3)\n",
        "        plt.imshow(output)\n",
        "\n",
        "        i = 1\n",
        "        image = transforms.ToPILImage()(images[i])\n",
        "        plt.subplot(3,3,3*i+1)\n",
        "        plt.imshow(image, 'gray')\n",
        "        mask = transforms.ToPILImage()(masks[i])\n",
        "        plt.subplot(3,3,3*i+2)\n",
        "        plt.imshow(mask)\n",
        "        output = transforms.ToPILImage()(outputs[i])\n",
        "        plt.subplot(3,3,3*i+3)\n",
        "        plt.imshow(output)\n",
        "\n",
        "        i = 2\n",
        "        image = transforms.ToPILImage()(images[i])\n",
        "        plt.subplot(3,3,3*i+1)\n",
        "        plt.imshow(image, 'gray')\n",
        "        mask = transforms.ToPILImage()(masks[i])\n",
        "        plt.subplot(3,3,3*i+2)\n",
        "        plt.imshow(mask)\n",
        "        output = transforms.ToPILImage()(outputs[i])\n",
        "        # output=np.array(output)\n",
        "        # output[output>75]=255\n",
        "        # output[output<=75]=0\n",
        "        plt.subplot(3,3,3*i+3)\n",
        "        plt.imshow(output)\n",
        "        # print(np.array(output))\n",
        "        # print(output.max())\n",
        "        # print(output.mean())\n",
        "        break\n",
        "\n",
        "# Restore training\n",
        "model.train()\n",
        "print('Finished Evaluation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsSUZOpQDq3j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}